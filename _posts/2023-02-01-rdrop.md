---
layout: post
title: R-Drop: Regularized Dropout for Neural Networks
<!-- date: 2023-01-19 -->
<!-- description: jekyll chirpy  -->
math: true
mermaid: true
<!-- featuredImage: null -->
<!-- img: null -->
tags: deeplearning nlp transformer 
categories: nlp translation
<!-- image: -->
<!--   src: /assets/img/Chirpy-title-image.jpg -->
<!--   width: 1000   # in pixels -->
<!--   height: 400   # in pixels -->
<!--   alt: image alternative text -->
---
<!-- # R-Drop: Regularized Dropout for Neural Networks -->

## Abstract 
Dropout은 deep neural networks을 train 할 때 강력하고 넓은 범위에  regularize 할 수 있는 방법 입니다. 하지만 random 적인 요소로 인해서 train과 inference 과정 사이에 inconsistency가 있을 수 있습니다. 본 논문에서는 regularize dropout을 위한 간단한 consistency training strategy인 R-Drop을 소개 합니다. Dropout을 통해서 생성된 submodels의 output distributions을 consistent하게 합니다. 구체적으로 dropout을 통해서 생성된 두개의 sub models sampled을 양방향 KL-divergence을 통해서 output distributions의 차이를 minimize 합니다. 이론적 분석을 통해서 R-Drop이 inconsistency을 감소시킬 수 있음을 나타냅니다. 

## 1 Introduction
최근 deep elarning은 다양한 분야에서 놀랄만한 성공을 거두었다. deep neural network을 훈련할 때 regularizaiton techniques은 over fitting을 방지하고, 일반화 성능을 증가시키는데 필요불가결의 존재이다. dropout technique이 가장 넓게 쓰이지만 train할 때만 일정 비율의 hidden units을 dropping 하므로 training과 inference 단계에서 무시할 수 없는 inconsistency과 존재한다. inconsistent한 hidden states에 L2 regularization 하는 방법이 존재하지만 넓게 사용되지 않습니다.  

본 논문에서는 dropout에 유발되는 training inconsistency을 regularize하는 간단하면서도 더 효과적인 대안 R-Drop을 제안합니다. 구체적으로 각 mini-batch training에서 각 data sample을 두 번 forward pass 합니다. 각 pass는 randomly dropping out에 의해 다른 sub model distribution이 됩니다. R-Drop은 두 개의 distribution을 bidirectional Kullback-Leibler divergence을 이용해서 minimizing 합니다. 이러한 방식을 사용하면 training과 inference estage의 inconsistency가 완화됩니다. 기존의 training 과정과 비교해보면 R-Drop은 추가적인 구조적인 변경 없이 오직 KL-divergence loss을 추가한 것 뿐입니다.



![1.png](/assets/img/rdrop/1.png)



R-Drop regularization은 간단하지만 매우 효과적임을 광범위한 18개의 datasets 5개의 tasks에서 발견할 수 있었습니다.(30.91 BLEU score on WMT14 English→German and 43.95 on WMT14 English→French translation tasks)

main contribution은 아래와 같이 요약할 수 있습니다.
* 간단하면서도 효과적인 regularization 방법인 R-Drop을 제안함. 다른 종류의 deep models을 train할 때 universally하게 적용 가능합니다.
* 이론적으로 R-Drop이 dropout based models에서 training과 inference시의 inconsistency을 줄일 수 있음을 보여줍니다.
* 18개의 dataset에서 4 NLP와 1 CV tasks을 통해서 R-Drop 강력한 성능을 달성할 수 있음을 보입니다.

## 2 Approach
R-Drop의 전반적인 구조는 Figure 1에 나와 있습니다. detail한 부분에 대해서 설명하기 앞서 필요한 정보를 설명 합니다.
training dataset D는 아래와 같이 주어집니다. n은 training samples의 수, $(x_i,y_i)$는 labeled data pair, $x_i$는 input data, $y_i$는 label을 의미 합니다.  예시로 machine translation에서 $x_i$는 source language sentence, $y_i$는 target language sentence라고 할 수 있습니다.
$$D = \{(x_i,y_i)\}_{i=1}^n$$
아래 식은 훈련하고자 하는 모델의 probability distribution(model output)을 의미 합니다.
$$P^w(y|x)$$
이후 두 개의 distributions $P_1$, $P_2$ 사이의 KL divergence는 $D_{KL}(P_1||P_2)$로 나타 냅니다.

### 2.1 R-Drop Regularization
deep learning model의 main learning 목적은 아래와 같은 negative log-likelihodd loss function을 최소화 하는 것입니다.
$$\begin{align}L_{nll}=\frac{1}{n}\sum_{i=1}^n-logP^w(y_i|x_i)\end{align}$$
이때 딥러닝 네트워크는 over-fitting 하는 경향이 있는데 이를 줄이기 위해서 dropout과 같은 regularization methods를 채택 합니다. 하지만 training stage에서는 randomly dropped 된 sub model을 이용하고, inference phase에서는 dropout 없는 full model을 이용하므로 inconsistency가 발생하게 됩니다. 이를 막기 위해 논문에서는 R-Drop을 제안하고 있습니다.

구체적으로 input data $x_i$를 각 training  step에서 forward pass를 두 번 반복합니다. 그러면 두개의 $P_1^w(y|x)$, $P_2^w(y|x)$ distributions of the model predictions을 얻을 수 있습니다. 그러면 training step에서 이 두 distribution에 bidirectional Kullback Leibler divergence를 적용해서 minimizing 하려고 합니다.
$$
\begin{align}
L^i_{KL}=\frac12(D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i))+D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i))) 
\end{align}$$
두 개의 forward passes에 대한 기본적인 negative log-likelihood learning objective $L^i_{NLL}$는 아래와 같이 나타낼 수 있습니다.
$$\begin{align}L^i_{NLL}=-logP_1^w(y_i|x_i)-logP_2^w(y_i|x_i)\end{align}$$
최종 training objective는 $L^i$를 minimize 하는 것입니다.
$$
\begin{equation}
\begin{split}
L^i = L^i_{NLL} + \alpha \cdot L^i_{KL} = -logP_1^w(y_i|x_i)-logP_2^w(y_i|x_i)  \\ + \frac{\alpha}2(D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i))+D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i)))
\end{split}
\end{equation}
$$

$\alpha$는 $L^i_{KL}$을 조정하기 위한 coefficient weight 입니다. 

### 2.2 Training Algorithm


![2.png](/assets/img/rdrop/2.png)


R-Drop의 전체적인 training algorithm은 위의 Algorithm1에 나와 있습니다. Line 3-4는 model에 forward 해서 $P_1^w(y|x)$, $P_2^w(y|x)$ output distributions을 얻는 것을 나타내고, Line 5-6는 두 distribution 사이에서 NLL 와 KL-divergence를 얻는 것을 나타 냅니다. 여기서 중요한 것은 forward를 두번 반복하는 것이 아닌 input data x를 복사해서 batch size 차원에서 concatenate해서 training cost를 줄입니다. 마지막으로 Line 7에서 parameters을 식 (4)의 loss를 이용해서 update 합니다. 기존의 training 방법과 비교하면 논문의 implementation은 batch size를 두배로 하는것과 비슷하고, computational cost가 각 step 마다 증가한다는 한계가 있습니다.

### 2.3 Theoretical Analysis
R-Drop의 regularization effect에 대해서 분석합니다. $h^l(x) \in \mathbb{R^d}$은 input vector x에 대한 l-th layer의 output을 의미 합니다. $\varepsilon \in \mathbb{R}^d$은 random vector를 의미하며, 각 차원은 독릭적으로 Bernoulli distribution B(p)를 따릅니다.
$$\varepsilon_i^l = 
\begin{cases}
1, \quad with \; probability \; p, \\
   0, \quad with \; probability \; 1-p
\end{cases}	
$$

$h^l(x)$에 dropout이 적용을 하면 다음과 같이 표현할 수 있습니다. $h^l_{\varepsilon^l}(x) = \frac1p\varepsilon^l \bigodot h^l(x)$ ($\bigodot$은 elemtwise를 의미)
parameter w를 가지고 있는 neural network에 dropout을 적용 후 output distribution은 다음과 같습니다.
$P_\varepsilon^w(y|x):=softmax(linear(h_{\varepsilon^L}^L(\cdot\cdot\cdot h_{\varepsilon^1}^1(x_{\varepsilon^0})))))$, where $\varepsilon = (\varepsilon^L,\cdot\cdot\cdot,\varepsilon^0)$

R-Drop은 다음과 같은 제한된 최적화 공식을 해결하는 것으로 나타낼 수 있습니다.
$$\begin{align}
&\min_w \frac1n\sum_{i=1}^n\mathbb{E_\varepsilon}[-logP_\varepsilon^w(y_i|x_i)],\\
&s.t. \frac1n \sum_{i=1}^n \mathbb{E}_{\varepsilon^{(1)}, \varepsilon^{(2)}} [D_{KL}(P_{\varepsilon^{(1)}}^w(y_i|x_i)||P_{\varepsilon^{(1)}}^w(y_i|x_i))] \leq\epsilon.
\end{align}$$

R-Drop optimizes는 식 (5)와 식 (6)를 확률적 방법으로 최적화 하는 방법입니다. 좀더 자세히 살펴보면, Bernoulli distribution으로 부터 생성된 두 개의 랜덤 벡터 $\varepsilon^{(1)}$, $\varepsilon^{(2)}$ (앞에서 twice forward pass를 하면서 작용하는 dropout을 나타냄)와 training instance $(x_i,y_i)$와 parameter w를 식 (4)로 부터 $\triangledown_w\mathcal L^i$ stochastic gradient 해준다.

dropout에는 training과 inference 사이의 inconsistency 문제가 존재합니다. 구체적으로 보면 training objective는 sub model들의 average loss들을 최소화 하는 것입니다(식 (5) 의미, full model은 $\tilde{P}^w(y|x)$ inference에 사용). 거기서 식 (6)의 제약조건을 이용하면 training과 inference 사이의 gap을 줄일 수 있습니다. (자세한 증명은 Appendix B)

### 2.4 Discussion
R-Drop과 가장 유사한 연구는 ELD와 FD가 있습니다. 그들 또한 consistency training with drop에 관해서 연구하지만, R-Drop은 그들과 다른점이 있습니다.
1. gap control을 다른 관점에서 합니다. R-Drop과 FD는 sub models 들끼리의 gap을 줄이는데 반해서, ELD는 직접 train과 inference의 gap을 줄입니다. sub models 끼리 gap을 줄이는 것의 장점은 FD에 증명되어 있습니다.
2. regularization efficiency가 다릅니다. ELD는 오직 하나의 sub-model에서만 back-propagates를 합니다. 그에 반해서 R-Drop는 두 sub models 모두에서 back-propagates를 합니다.
3. regularization effect가 다릅니다. ELD, FD는 L2 distance loss function을 사용합니다. 이것은 main training obejective인 NLL loss와 거리가 있습니다. main objective에는 log-softmax 함수가 적용되는데 이로 인해서 L2 distance 함수로 minimize하면 vector space내의 분포를 정확하게 표현하지 못합니다. KL divergence는 이와 다르게 제대로 표현 가능합니다.(Appendix C.4 참조)

## 3    Experiments
실험은 5가지 tasks에 관해서 진행함. 4 NLP, 1 CV. neural machine translation (6 datasets), abstractive summarization (1 datasets), language understanding (8 datasets), language modeling (1 dataset), image clasification (2 datasets). 편의를 위해서 R-Drop을 RD로 표기합니다.

### 3.1    Application to Neural Machine Translation
먼저 NMT tasks에 관해서 평가를 진행 합니다. 실험은 low-resource와 rich-resource에 관해서 진행 합니다.

####  Datasets
low resource는 IWSLT14 English <-> German, English <-> Spanish, IWSLT17 English <-> French, English <-> Chinese을 사용 하였습니다.
rich resource는 WMT14 English -> German, English -> French을 사용 하였습니다.

#### Model & Training
model은 transformer은 사용 하였습니다. $\alpha$ = 5로 세팅.

#### Results
평가는 BLEU Scores를 사용 하였습니다. IWSLT 성능은 Table 1에, WMT 결과는 Table 2에 나타 냈습니다.


![3.png](/assets/img/rdrop/3.png)



![4.png](/assets/img/rdrop/4.png)



8개의 IWSLT에서 base 보다 2.0 BLEU 향상을 확인할 수 있습니다.  WMT의 score는 기존 SOTA model인 BERT-fused NMT(monolingual data 활용)을 뛰어 넘을 수 있었습니다.

### 3.2    Application to Language Understanding
#### Dataset
평가는 pre-trained models에 R-Drop을 적용하여 fine-tunning한 모델을 GLUE dataset에서 진행 하였습니다.

#### Model & Traning
BERT-base, RoBERTa-large을 pre-trained models로 사용합니다. $\alpha$는 {0.1, 0.5, 1.0} 다양한 세팅으로 진행 하였습니다. 

#### Results
STS-B는 Pearson correlation, CoLA는 Matthew's correlation, 나머지 tasks는 Accuracy로 평가를 진행 하였습니다. 결과는 아래 Table 3에 나와 있습니다.



![5.png](/assets/img/rdrop/5.png)



결과를 보면 R-Drop을 사용한 방법은 기존 baselines인 BERT-base, RoBERTa-large에 비해서 1.21 points 와 0.80 points가 상승한 것을 확인할 수 있습니다.

### 3.3    Application to Summarization
#### Dataset 
CNN/Daily Mail dataset을 사용 하였습니다.
#### Model & Training
pre-trained 된 sequence-to-sequence BART model을 backbone으로 사용 하였고 fine-tune을 R-Drop을 적용해서 진행 하였습니다. $\alpha$ 는 0.7로 설정 하였습니다. 
#### Results 
평가는 ROUGE F1 score로 진행 하였습니다. 실험 결과는 아래 Table 4와 같습니다.



![6.png](/assets/img/rdrop/6.png)



결과 base 모델보다 0.3 points 증가, SOTA를 달성할 수 있었습니다.

### 3.4    Application to Language Modeling
#### Dataset
Dataset은 Wikitext-103 dataset을 사용 하였습니다.

#### Model & Training
model은 두가지를 사용 했는데 하나는 기본적인 Transformer decoder model이고, 다른 하나는 Adaptive Input Transformer(adaptive input embeddings)입니다. $\alpha$는 1.0으로 세팅 하였습니다.

#### Results
평가 방법은 perplexity를 사용 하였습니다. 결과는 아래의 Table 5와 같습니다.

![7.png](/assets/img/rdrop/7.png)

결과를 보면 base로 사용한 두 모델 모두에서 R-Drop을 적용 시 성능이 향상한 것을 확인할 수 있습니다. 

### 3.5    Application to Image Classification
#### Dataset
이미지 분류를 위한 데이터 셋은 CIFAR-100, ILSVRC-2012 ImageNet dataset을 사용 하였습니다.

#### Model & Training
모델은 Vision Transformer(ViT)를 사용 하였습니다. pre-trained model을 fine-tunning 하는 과정에서 R-Drop을 적용 하였습니다. 이때 $\alpha$는 0.6으로 설정 하였습니다.
#### Results
평가는 Accuracy로 이루어 졌습니다. 결과는 아래 Table 6에 나와 있습니다.

![8.png](/assets/img/rdrop/8.png)

결과를 살펴보면 성능이 증가한 것을 확인할 수 있습니다.

## 4    Study
이 섹션에서는 R-Drop method에 대해 좀 더 이해하기 위해서 다른 관점에서 광범위한 연구를 진행하였습니다. 분석 실험은 IWSLT 14 De -> En translation task에서 진행 하였습니다.

### 4.1    Regularization and Cost Analysis
먼저 R-Drop의 정규화 효과를 보이고 training cost에 대한 잠제적 한계를 살펴 봅니다. training/valid loss, valid BLEU를 training update number와 training time에 따라서 plot 합니다.
plot한 curves는 아래 Figure 2에 나와 있습니다.

![9.png](/assets/img/rdrop/9.png)

우리는 다음과 같은 사실을 관측할 수 있습니다.
1. training 과정을 살펴보면 transformer가 R-drop에 비해서 빠르게 over-fitting되고, train과 valid loss 간의 gap이 큰 것을 확인할 수 있습니다. 이것은 R-Drop의 regularization이 잘 작동함을 의미 합니다.
2. training stage의 초반부에 Transformer는 BLUE score가 빠르게 증가 합니다. 그러나 빠르게 나쁜 최적점으로 수렴 됩니다. 이와 비교해서 R-Drop는 천천히 BLEU가 증가 하지만, 더 뛰어난 성능을 달성합니다. R-Drop이 training cost를 증가 시키지만 이를 무시해도 될 정의 성능증가가 있음이 그림상에서 보여지고 있습니다. 

### 4.2    k-step R-Drop
기존에는 각 step마다 R-Drop을 적용 하였지만 training efficiency를 높이기 위해서 k-step 마다 R-Drop을 적용 했을때의 효과를 알아 봅니다. 그에 대한 결과는 위의 Figure 3에 나와 있습니다.  결과를 살펴보면 k-step이 높을수록 수렴이 빠른것은 확인할 수 있지만, 더 나쁜 최적점에 도달하는 것을 볼 수 있습니다. 

### 4.3    m-time R-Drop
논문에서 제안한 방법은 두 distributions인 $P_1^w(y|x)$ $P_2^w(y|x)$을 regularizes 하는 방법 입니다. 2개대신 m개를 regularizes 하면 성능이 어떻게 될지에 대해서 측정하였습니다. KL divergence에 관한 식은 아래와 같이 수정할 수 있습니다.
$$
L_{KL}=\frac{\alpha}{m * (m-1)}\Sigma_{i, j \in 1,\cdot \cdot \cdot, m}^{i \not = j} D_{KL}(P_i^w(y|x)||P_j^w(y|x)) 
$$

이를 이용해서 m=3에 대해서 실험을 진행해 본 결과 37.30을 얻을 수 있었고 m=2 일때는 37.25 였던 결과에 비해서 그렇게 성능 증가폭이 크지 않음을 확인할 수 있습니다. m=2 일때 이미 충분히 강력함을 알 수 있습니다.

### 4.4    Two Dropout Rates
IWSLT translation에서 모델의 dropout은 0.3 입니다. 이때 dropout의 영향에 대해서 알아보기 위해서 {0.1, 0.2, 0.3, 0.4, 0.5}의 dropout 집합에서 2 개씩 선택하여서 실험을 진행 하였습니다. 결과는 아래 Figure 4에 나와 있습니다.


![10.png](/assets/img/rdrop/10.png)

결과를 살펴보면
1. (0.3, 0.3)에서 가장 뛰어난 성능을 나타 냈습니다.
2. (0.3 ~ 0.5)의 합리적인 범위 안에서 큰 성능차이 없이 뛰어난 성능을 보였습니다. 

### 4.5    Effect of Weight
KL-divergence loss weight인 $\alpha$에 대해서 조사를 진행 하였습니다. 기존 실험은 $\alpha$ = 5 에 관해서 진행 하였는데, 본 실험은 {1, 3, 5, 7, 10}의 범위에서 선택 하였습니다. 결과는 아래 Table 7에 나와 있습니다.

![11.png](/assets/img/rdrop/11.png)

결과를 살펴보면 너무 작거나 너무 클 떄 보다 5일때 제일 성능이 좋은것을 확인할 수 있습니다. 이러한 $\alpha$는 각 task에 따라서 최적의 값이 다른데 data size나 model size가 다르기 때문입니다.

## 5    Related Work

### Regularization Methods
over fitting을 피하기 위해 많은 regularization이 제안 되었습니다. weight decay, dropout, normalization, adding noise, layer-wise pre-training and initialization, label-smoothing. 이 중 dropout이 평범한 cost로 좋은 성능을 낼 수 있어서 가장 유명합니다.

### Consistency Training
앞에서 살펴봤던 ELD, FD, Cutoff에 대해서 이야기 하는데
ELD, FD는 둘다 L2 loss function을 사용 하였고, ELD는 sub model(train 상태)와 inference 모델 간의 consistency만 신경쓰고, FD는 submodels 들간의 consistency를 고려 합니다.
cutoff는 R-Drop과 비슷하게 KL-divergence를 쓰지만, sub models들 간이 아니라 문장에 빈칸을 줘서 빈칸인 sentence와 full sentence간의 consistency training을 진행 합니다.

### Self-distillation
두개의 다른 models의 output distributions 간의 KL-divergence를 최소화 하는 것은 knowledge distilation의 student, teacher 방식과 비슷하다고 볼 수 있습니다. 다른 점이라면 R-Drop은 self knowledge distillation, 추가적인 parameter가 필요 없다는 점입니다.

## 6    Conclusions and Future Work
본 논문에서는 간단하면서도 효과적인 consistency training method, R-Drop를 제안 하였습니다. 실험결과 R-Drop은 미리 훈련된 강력한 모델뿐 아니라 large-scale datasets으로 훈련 할 때도 잘 작동하였습니다. computational resources의 제한으로 인해 몇몇 tasks는 downstream task에만 R-Drop을 적용 하였지만, pre-training을 후일에 진행할 예정 입니다.
