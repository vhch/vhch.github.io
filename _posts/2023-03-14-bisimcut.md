---
layout: post
title: Bi-SimCut A Simple Strategy for Boosting Neural Machine Translation
date: 2023-03-14
<!-- description: jekyll chirpy  -->
math: true
mermaid: true
<!-- featuredImage: null -->
<!-- img: null -->
tags: deeplearning nlp transformer 
categories: nlp translation
<!-- image: -->
<!--   src: /assets/img/Chirpy-title-image.jpg -->
<!--   width: 1000   # in pixels -->
<!--   height: 400   # in pixels -->
<!--   alt: image alternative text -->
---

# Abstract 
Bi-Simcut은 간단하지만 효과적인  training strategy 입니다. 이것은 두 단계로 구성 됩니다. 1. bidirectional pretraining 2. unidirectional finetuning. 두 단계 모두 original, cutoff sentence paris간의 output distributions의 consistency를 유지하게 하는 정규화 방법 SimCut을 적용 합니다. 이떄 back-translation으로 생성한 extra dataset이나 large-scale pretrained model은 사용하지 않고, Bi-SumCut은 강력한 translation performacne를 달성 하였습니다. Sim-Cut은 새로운 방법이 아니고, Cut-off (Shen et al. 2020)를 간단화하고 NMT에 적용한 방법 입니다.

# 1 Introduction 

## 연구 필요성
* 기존 연구 simcut은 뛰어난 성능을 가짐
* 하지만 연구에서 소개된 Hyperparameter 4개의 적절한 값을 찾는것은 지루하고 많은 시간이 소모됨.

## 연구 목표 및 내용
* NMT 교육을 위한 전략을 제공
	* 간단하고 재현하기 쉽지만 효과적
	* cutoff 참조
	* virtual adversarial regularization 참조
* SimCut 제안
	* 새로운 방법 x
	* Cutoff에서 제안된 Token Cutoff를 간소화 함
	* bidirectional backpropagation in KL regularization이 key role임
* Bi-SimCut 제안
	* SimCut은 perturbation based이면서 augmentation 방법임
	* 두 단계로 training strategy가 구성됨
		* bidirectional pretraining with SimCut
		* unidirectional finetuning with SimCut
## 연구 Contribution
- SimCut 제안
	- perturbation based method로 간주 가능
	- pretrained language models mBART에 대한 호환성을 보임
- Bi-SimCut 제안
	- bidirectional pretraining, unidirectional finetuing with SimCut으로 구성
- 5가지 translation benchmark에서 인상적인 개선
	- Transforemr model with Bi-SimCut을 이용

# 3 Datasets and Baseline Settings

## Datasets
- IWSLT14 en <-> de (low-resource)
- WMT17 zh -> en  (high-resource)


![1.PNG](/assets/img/bisimcut/1.PNG)

## Settings
- Base model = Transformer
- label smoothing = 0.1
- max tokens per batch = 4096
- Adam optimizer with Beta (0.9, 0.98)
- 4000 warmup steps
- learning rate = 5e-4
- dropout rate = 0.3
- beam size = 5
- length penalty 1.0



# 4 Bi-SimCut
##  4.1 SimCut: A Simple Cutoff Regularization for NMT

- 기존 cut off는 ($p_{cut}$, $\alpha$, $\beta$, N)의 parameter를 일일히 찾아야 함
- 이러한 부담을 줄이기 위해서 SimCut 제안
- problem formulation는 Virtual Adversarial Training (VAT)에서 영감을 얻음
	- KL-based adversarial regularization
	- adversarial perturbations $\delta_x \in \mathbb R^{d \times I}$, $\delta_y \in \mathbb R^{d \times J}$을 original sampels에 더한 것과 original samples의 차이를 줄임
	- $KL(f(e(x), e(y); \theta , \|\| f(e(x) + \delta_x, e(y) + \delta_y; \theta))$
- Token Cutoff로 perturbation 생성
	- 기존 VAT는 gradient 방식으로 perturbation 생성	 
	- 각 sentence pair (x, y)에서 하나의 cutoff smaple ($x_{cut}$, $y_{cut}$) 생성
	- Training objective of Simcut
		- $L_{simcut}(\theta) = L_{ce}(\theta) + \alpha L_{simkl}(\theta)$
		- $L_{simkl}(\theta)$ =  $KL(f(x, y; \theta \, \|\| f(x_{cut}, y_{cut} ; \theta))$
		- 두개의 hyper-parameters $\alpha$, $p_{cut}$만 존재
		- VAT는 KL의 오른쪽항에 대해서만 back propagation, BiSimCut은 왼, 오른쪽 모두 back propagation

## 4.2 Analysis on SimCut
### 4.2.1 How Does the Simplification Affect Performance?


![2.png](/assets/img/bisimcut/2.PNG)


- 기존 방법보다 뛰어난 성능 달성


![3.png](/assets/img/bisimcut/3.PNG)


-  training cost에 따른 성능 비교
	- VAT over fitting 발생
	- SimCut은 much more training time to converge이지만 over fitting 덜 발생
	- Token Cutoff costs about 148 seconds per epoch (N=1)
	- Simcut costs about 128 seconds per epoch

### 4.2.2 How Does the Bidirectional Backpropagation Affect Performance?



![4.png](/assets/img/bisimcut/4.PNG)


- KL 항에서 양쪽항을 back propagation 할 떄의 효과
- Bi-backpropagation시 성능 증가

### 4.2.3 Performance on Perturbed Inputs
- Simcut은 perturbation method로 간주 가능
- 기존의 perturbation 방법과 비교
- 확률 p ~ (0.00, 0.01, 0.05, 0.10)로 replace or drop된 문장으로 실험 진행

![5.png](/assets/img/bisimcut/5.PNG)



![6.png](/assets/img/bisimcut/6.PNG)


- SimCut이 perturbed test set에서 가장 robustness 함

### 4.2.4 Effects of $\alpha$ and $p_{cut}$
- $\alpha \in \{1, 2, 3, 4, 5\}$
- $p_{cut} \in \{0.00, 0.05, 0.10, 0.15, 0.20\}$



![7.png](/assets/img/bisimcut/7.PNG)

### 4.2.5  Is SimCut Compatible with the Pretrained Language Model?
- Backbone model : mBART
- dataset
	- IWSLT14 de->en
	-  only remove the duplicated sentence pairs following mBART50

- Transformer
	- randonly initialized
	- trained from scratch
	- configurations in Section 3
- mBART
	- directly finetuned from mBART
- mBART with SimCut
	- finetuned from mBART with SimCut
	
![8.PNG](/assets/img/bisimcut/8.PNG)


## 4.3 Training Strategy : Bidirectional Pretrain and Unidriectional Finetune

- English -> German dataset 존재
- English + German -> German + English로 먼저 pre train
- 이후 English -> German으로 fine tunning



![9.PNG](/assets/img/bisimcut/9.PNG)



![10.PNG](/assets/img/bisimcut/10.PNG)

# 5 Standard Resource Scenario
## 5.1 Dataset Description and Model Configuration
- Dataset : WMT14 English-German (4.5M parallel sentence)
- Validation set : newstest2012, newstest2013
- Test set : newstest2014
- shard dictionary with 52K bpe
- Transformer Big model
- cross entropy loss with label smoothing rate 0.1
- max tokens per batch to be 4096
- Adam optimizer with Beta (0.9, 0.98)
- 4000 warmup steps
- inverse square root learning rate scheduler with initial learning rates 1e-3
- decrease the learning rate to 5e-4
- dropout = [0.3, 0.2, 0.1]
- beam size = 4
- length penalty 0.6

## 5.2 Results
- $p_{cut}$ = 0.05



![11.PNG](/assets/img/bisimcut/11.PNG)

# 6 High Resource Scenario
## 6.1 Dataset Description and Model Configuration
- Dataset : WMT17 Chinese-English dataset (20.2M training sentence pairs)
- Validation set : newsdev2017
- Test set : newstest2017
- 32K BPE vocabularies
- Transformer Big model

## 6.2 Results


![12.PNG](/assets/img/bisimcut/12.PNG)
