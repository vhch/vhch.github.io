---
layout: post
title: BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation 리뷰
date: 2023-01-19
<!-- description: jekyll chirpy  -->
math: true
mermaid: true
<!-- featuredImage: null -->
<!-- img: null -->
tags: deeplearning nlp transformer 
categories: nlp translation
<!-- image: -->
<!--   src: /assets/img/Chirpy-title-image.jpg -->
<!--   width: 1000   # in pixels -->
<!--   height: 400   # in pixels -->
<!--   alt: image alternative text -->
---

## 1. Introduction
Pre-trained language models들은 large-scale unlabeled data에서 훈련합니다. ELMo, BERT, XLNET, XLM 등이 존재하고 다양한 NLP tasks에서 뛰어난 성능을 보이고 있습니다. 뛰어난 성능을 보이는 BERT의 pre-trained 모델을 translation model에 활용하는 몇가지의 예가 존재합니다.

1. Model의 Encoder 부분을 random하게 초기화 하는 대신 pre trained된 BERT의 parameter로 초기화 하는 방법
2. BERT의 output을 encoder의 각 layer에 활용하는 방법

이 논문에서는 간단하게 pre trained language model의 output을 NMT 시스템의 input으로 활용 함으로써 IWLST'14, WMT14에서 back translation 없는 상황에서  state-of-the-art를 달성 하였습니다.

또한 다양한 pre-trained langauge model을 평가하고, specialized pre-trained bilingual model을 구현 하였습니다. 추가로 두 가지 개선점을 도입 하였습니다.

1.  stochastic layer selection
2.  dual-directional training

Fig.1에 그에 대한 overview를 나타냅니다.

![1.PNG](/assets/img/bibert/1.PNG)

## 2.  Contextualized Embeddings for NMT
### 2.1 Method
이 섹션에서는 pre-trained language model의 last layer(contextualized embeddings)을 활용하여 NMT model을 구성할 때의 효과를 조사 합니다.

논문의 basic NMT models은 6 layer의 transformer translation models을 사용합니다. 구체적으로 source sentence를 frozen pre-trained language model에 넣어서 final layer를 NMT encoder의 embedding layer에 활용 합니다.

먼저 처음으로 간단하게 contextualized embeddings을 이용할 때 얼마나 성능이 향상되는지 살펴보고, 다양한 pre-trained language models이 NMT models에 미치는 영향을 알아 봅니다.

### 2.2  Existing Pre-Trained Models
먼저 네 개의 유명한 pre-trained models(two monolingual models, two multilingual models)을 NMT모델에 활용한 것을 살펴 봅니다.

* **ROBERTA** : BERT보다 더 큰 dataset에서 dynamic masking으로 바꾸고, next sentence prediction을 제거한다.
* **GottBERT** : 독일어 버전의 RobertA 모델이다.
* **MBERT(cased)** : 104개의 언어로 pre-trained된 multilingual Bert이다.
* **XLM-R(base)** : transformer-based에서 100개의 언어로 masked language model train한 모델이다.

### 2.3  How Do Pre-Trained LMs Affect NMT?

* **Dataset** : low resource인 IWLST14에서 실험을 진행한다. IWLST' 14 English-German dataset은 160k parallel bilingual sentence pairs로 구성되어 있습니다.
* **Settings** : 모델은 six layer, FFN dimension size = 1024, attention heads = 4로 구성된 transformer_iwslt_de_en을 사용합니다. embedding dimension은 pretrained language models에 맞추기 위해서 768을 사용한다. Evaluation metric는 일반적으로 사용되는 tokenized  BLEU  (Papineni  et  al.,  2002)을 사용 합니다.

* **Observations** : IWLST' 14에 대한 결과는 Table 1.에 나와있다.
<br> ![2.PNG](/assets/img/bibert/2.PNG)
<br> pre-trained language models을 활용 하려면 NMT model의 vocabulary가 pre-trained models과 동일해야 한다. random은 pre trained의 last layer을 사용하는 대신 NMT의 embedding layer을 random하게 초기화 하는 방식(기존의 방식과 동일)인데, 각 Language model마다 vocabulary가 다르기 때문에 성능 향상이 vocabulary차이 때문이 아님을 보여주기 위해서 random 방식의 실험도 같이 진행한다.decoder vocabulary size는 8K로 고정한다. 이에 대한 detail은 2.5에 나와있다. random 초기화한 결과는 대부분 비슷한 값을 가진다. embedding layer을 contextualized embeddings으로 교체한 결과 GOTTBERT와 ROBERTA의 성능은 눈에 띄게 증가 하였다. 그러나 MBERT, XLM-R은 약간 증가 하거나 오히려 성능이 감소하는 것을 확인할 수 있습니다.

* **Curse of Multilinguality** : MBERT와 XLM-R 에서 성능이 하락하는 것을 볼 수 있는데 이는 multilinguality 때문인 것으로 보고 있다. low-resource language performance는 higher resource languages를 추가하면 증가합니다. 하지만 higher resource의 성능은 내려가게 된다. English와 German은 higher resource에 속하게 되고 성능이 하락하게 된다. 이러한 원인은 model의 parameter의 capacity가 부족해서 그렇습니다.

### 2.4  Customized Pre-Trained LM
앞에서 보았듯이 monolingual language models은 machine translation systems의 성능을 증가시키는 것을 확인할 수 있었지만 machine translation는 본질적으로 bilingual task 입니다.  따라서 논문에서는 monolingual data가 아닌 source language와 target language의 mixture data로 train 하는 방법을 가설로 제기 합니다. 이러한 bilingual pre-trained language model을 **BIBERT**라고 제안합니다. 이러한 BIBERT는 ROBERTa 구조를 베이스로 합니다. $BIBERT_{EN-DE}$는 GOTTBERT의 German data와 추가로 146GB English texts를 활용 합니다.

### 2.5  Vocabulary Size Selection
encoder의 vocabulary size는 pre-trained models에 맞춰서 고정 되어 있지만, decoder의 size는 선택할 수 있습니다. low-resource machine translation에서는 decoder vocabulary size에 따라서 매우 민감합니다.  Gowda and May (2020)는 large grid search를 통해 8K BPE에서 가장 잘 작동함을 밝혔습니다. IWSLT' 14 (160K parallel sentences) dataset에서도 적합한지 확인하기 위해 decoder vocabulary sizes (8K, 16K, 24K, 32K)에 대해서 확인 합니다.

![3.PNG](/assets/img/bibert/3.PNG)
<br>Figure 2.를 살펴보면 8K에서 가장 뛰어난 BLUE score 달성을 확인할 수 있습니다. 이후 후속 실험은 8K vocabulary sizes로 진행 합니다.

### 2.6 BIBERT Performance
* **$BIBERT_{EN-DE}$  results** : 앞의 Tabel 1. 에서 논문에서 제안한 모델은 EN -> DE에서 baseline보다 2.12 증가한 29.65 score 달성함을 확인할 수 있습니다. DE -> EN에서는 4.06 증가한 37.58 score를 확인할 수 있습니다. 이때 GOTTBERT와의 차이점은 extra English training data를 추가한 것 뿐이지만 1.3 BLUE point의 증가가 있었습니다.
* **Analysis :** BIBERT의 뛰어난 성능에 대해서 분석해 보면, BIBERT의 contextualized embeddings output이 GOTTBERT보다 German information을 풍부하게 갖고 있다 생각할 수 있다. 그리고 extra English data를 가짐으로써 더 나은 assist를 translation model에 제공한다. 또한 ROBERTA 보다 더 적은 English training data를 사용하지만 0.68 BLUE의 성능 향상이 있음을 알 수 있다. BIBERT의 뛰어난 성능에 대해서 부연 설명을 하자면
	<br>
	1. 비슷한 의미를 가지는 두 개의 언어에 대한 aligned embeddings for the tokens을 배울 수 있기 때문이다. 즉 source embeddings은 encoder에 aligned target embeddings에 대한 힌트를 제공할 수 있다.
	2. overlapping된 En-De sub-word units의 Embedding은 NMT encoder에 bilingual 정보를 활용하여 translation이 가능하게 한다.
<br>![4.PNG](/assets/img/bibert/4.PNG)

## 3.  Layer Selection
기존에는 pre-trained models의 last layer만 활용 했었는데, BERT의 각 layer는 다른 linguistic information을 capture 합니다. 그래서 더 많은 정보를 활용하기 위해서 last layer만 말고 다른 layer도 select 하는 방법에 대해서 알아 봅니다.

$\chi$를 source language sentences의 집합이라고 나타냅니다. 각 source sentence x $\in$ $\chi$, $H^i_B(x)$는 x를 pre trained language model에 넣었을때 $i_{th}$ layer의 contextualize embeddings을 의미 합니다. 이때 $H^i_B(x)$는 다음과 같은 조건, last layer에서 처음 layer 까지 방향으로 K개의 layer만 고려 합니다.
$$H^i_B(x), \,  \forall i \in [M - K + 1, M]$$, K는 hyperparameter, M은 pre-trained language model의 총 layers 수 입니다.

### 3.1  Stochastic Layer Selection
논문에서는 pre-trained language model의 더 많은 layer의 정보를 활용할 수 있는 stochastic layer selection을 제안함. 구체적으로 기존에는 last layer만 활용 했었는데, 각각의 batch에서 pre trained model의 layer중 랜덤하게  하나의 layer를 선택해서 NMT encoder의 input으로 사용함(Fig 3에 나타남).
![5.png](/assets/img/bibert/5.png)
sentence x로 부터 NMT encoder에 들어가는 input embeddings을 $H_E(x)$로 나타내며, 훈련과정 중 다음과 같이 정해집니다.

$$
H_E(x) = \int_{i=1}^K\mathbb{1}(\frac {i-1}K < p \leq \frac i K)H_B^{M-i+1}(x) \qquad (1)
$$

$\mathbb1$은 indicator function이고 p는 [0, 1] 균등하게 분포하는 random variable 입니다. 즉 train할 때 각 batch에서 p가 [0, 1]의 범위에서 random variable로서 정해지고 K개의 layer 중 $(\frac {i-1}K < p \leq \frac i K)$에 해당하는 하나의 layer만 선택해서 train이 진행 됩니다. 이후 inference step에서는 training에 사용했던 모든 layer output의 expectation 값을 input embedding으로 사용 합니다.
$\mathbb{E}_{p \sim uniform[0, 1]}[H_E(x)]$, 이 식은 inference 과정을 의미하고 식 1을 다음 식2와 같이 수정할 수 있습니다.

$$
H_E(x) = \frac 1 K\sum_{i=1}^K H_B^{M-i+1}(x) \qquad (2)
$$

### 3.2  Experiments and Results
Table 1에서 가장 뛰어난 성능을 모였던 $BIBERT_{EN-DE}$을 basis로 하여 후속 연구를 진행 합니다. 실험은 Section 2와 동일하게 IWSLT' 14 dataset으로 진행합니다.

![6.png](/assets/img/bibert/6.png)

Fig 4는 stochastic layer selection의 영향을 보여줍니다. K의 범위는 2~M으로 진행 하는데 $BIBERT_{EN-DE}$에서 레이어 수가 12개 이므로 M=12 입니다. K=1 이면 last layer만 select 하는것으로 section 2와 동일 하므로 제외 합니다. Fig 4를 살펴보면 모든 경우에서 K=1일때 보다 성능이 증가하는 것을 확인할 수 있습니다. K=8 일때 최대 성능을 가집니다.

## 4. One Model, Dual-Directional Translations
이 섹션에서는 평범한 one-way translation models과 다르게 dual-directional translation models에 대해서 설명 합니다. model이 En -> De De -> En 양방향으로 translate 할 수 있게 하는것이 목적입니다. 모델의 구조는 Section 3와 동일 합니다. 방법은 source sentence와 target sentence를 encoder, decoder에만 넣지 말고, 반대 방향과 정방향을 한번에 같은 model에서 train하는 것이 요지입니다. 이러한 방법의 motivation은 source, target sentences과 서로 contextualized representations을 향상 시켜줄 것이라고 기대하기 때문입니다. 또한 이러한 방법은 data augmentation 관점으로도 볼 수 있습니다.
이 방법의 장점은 다음 2가지 입니다.
1. 추가적인 bitexts 없이 성능 증가를 얻을 수 있습니다.
2. 모델 구조를 따로 수정할 필요 없이 data preprocessing의 작은 수정만 필요 합니다.

### 4.1  Dataset Preprocessing
dataset은 그대로 IWSLT'14 En->De 입니다. data preprocessing 과정은 아래 Fig 5에 나와 있습니다.
![7.png](/assets/img/bibert/7.png)

기존 src, tgt을 한 곳에서 concatenation후 shuffle하는 간단한 과정입니다. Decoder에서 joint English-German vocabulary size는 12k를 사용 하였습니다.

### 4.2  Fine-tuning
[Xu et al. (2021)](https://aclanthology.org/2021.adaptnlp-1.22/)에서 초기에 in and out of domain of data로 train후 in domain data로 fine-tuning시 성능이 증가한다는 점에서 영감을 받아서 초기에 Dual direction으로 train 시킨 후 mixed 데이터가 아닌 한 방향으로 fine tuning 한다는 것이 요점입니다.

### 4.3  Experiments and Results
기존에 decoder의 vocabulary size 8k로 진행 했었는데 그에 따른 대조군을 설정하기 위해 vocabulary size가 12k인 one-way translation model을 추가 합니다.
![8.png](/assets/img/bibert/8.png)

실험 결과는 위의 Table 3에 나와 있습니다.
기존의 one-way에서 dual-directional, fine-tuning, stochastic layer selection을 추가 할 수록 성능이 증가한다는 것을 나타내고 있습니다.

## 5.  High-Resource Scenario
### 5.1  Dataset and Training Details
앞의 실험들은 low resource인 IWSLT' 14 였고 이번 섹션에서는 high resource인 WMT' 14 English-German dataset에 관해서 실험을 진행 합니다. Model configuration은 big transformer with 4096 FFN dimension and 16 attention heads 입니다. 여기서 $BIBERT_{EN-DE}$과 dimensions을 맞춰주기 위해서 hidden size을 1024 -> 768로 바꿔 줍니다. 그리고 기존에는 low resource라 decoder의 vocabulary size를 8K와 12K를 사용 하였는데 여기서는 52K로 바꿔 줍니다.
### 5.2  Results
![9.png](/assets/img/bibert/9.png)
WMT' 14에서 제공되는 bitext만 사용한 실험결과는 위의 Table 4에 나와 있습니다. 특별하게도 dual-directional translation training이 앞의 low resource와 다르게 특별한 효과를 발휘하지 못하였습니다. 하나의 가능한 시나리오는 model capacity가 mixed domain data를 handle할 정도로 충분히 크지 못하였을 가능성이 있습니다. 여기서 중요한점은 실험이 기존 모델과 다르게 hidden size를 1024 -> 768로 훨씬 작은 사이즈를 했지만 기존의 결과들보다 우수한 성능을 달성한 점입니다.
## 6.  Related Work
생략
## 7.  Conclusion
본 논문에서는 BIBERT를 large amount of mixed text of the source and target language를 NMT model에 활용하고, pre-trained language model의 last layer를 간단하게 활용하는 것이 다른 방식의 pre-trained language model을 활용하는 방법보다 더 뛰어난 결과를 나타 냈습니다. 더욱이 stochastic layer selection method를 소개하여 성능 향상 있음을 보였습니다. 마지막으로 dual direction 방식이 성능 향상에 도움이 됨을 확인 했습니다.
