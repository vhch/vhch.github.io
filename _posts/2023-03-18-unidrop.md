---
layout: post
title: UniDrop A Simple yet Effective Technique to Improve Transformer without Extra Cost
date: 2023-03-14
<!-- description: jekyll chirpy  -->
math: true
mermaid: true
<!-- featuredImage: null -->
<!-- img: null -->
tags: deeplearning nlp transformer 
categories: nlp translation
<!-- image: -->
<!--   src: /assets/img/Chirpy-title-image.jpg -->
<!--   width: 1000   # in pixels -->
<!--   height: 400   # in pixels -->
<!--   alt: image alternative text -->
---

<!-- ![1.PNG](/assets/img/*/*.PNG) -->

# 1 Introduction

## 연구 필요성

- Over-parameterization and overfitting
    - Transformer의 다양한 훈련 전략 나옴
    - 위의 전략들이 효과적임에도 overfitting의 위험성 존재
    - overfitting을 피하기 위해서 Regularization methods 제안됨
        - weight decay
        - data augmentation
        - dropout
        - parameter sharing
- 다양한 dropout techniques 만으로 SOTA 달성 가능한가?
    - dropout은 추가적인 costs나 resource가 필요 x

## 연구 목표 및 내용

- Unidrop 제안
    - feature dropout : 전통적인 dropout
    - structure dropout : LayerDrop (Fan et al., 2020a)
        - layer 계층 통 채로 dropout
    - data dropout
        - randomly dropping out some tokens in an input sequence

- Evaluation
	- Neural machine translation with 8 translation datasets
	- Text classification task with 8 benchmark datasets

## 연구 Contribution

- UniDrop 제안
	- Three different dropout techniques을 통합함
	- 추가적인 cost나 사전지식 필요 x
- 이론적으로 증명
	- 세 가지 dropout은 Transformer에서 다른 역활을 하며 overfitting을 방지하고 robustness을 증가시킴
		- feature dropout
		- structure dropout
		- data dropout
- Transformer with UniDrop이 SOTA 달성
	- 광범위한 실험 결과 제시
		- sequence generation
		- classification task

# 3 UniDrop




## 3.1 Feature Dropout

![1.PNG](/assets/img/unidrop/1.PNG)

- FD-1 (attention dropout)
- FD-2 (activation dropout)
- FD-3 (query, key, value dropout)
- FD-4 (output dropout)

## 3.2 Structure Dropout
- Three structure dropouts
	- LayerDrop
	- DropHead
	- HeadMask
- DropHead, HeadMask
	- 최근 연구에서 multi-head attention mechanism이 attention head의 작은 부분에 의해 지배 받는다고 알려짐
	- 이를 방지하기 위해 특정 전체 heads를 drop하는 DropHead, HeadMask이 제안됨
- LayerDrop
	- higher-level and coarser-grained structure dropout
	- Drops some entier layers at training time
	- directly reduces Transformer model size

- LayerDrop을 structure dropout으로 채택


## 3.3 Data Dropout
- Data dropout
	- 사전 정의된 확률로 랜덤하게 문장의 some words를 제거

- Two-stage data dropout strategy 제안
	- Directly applying vanilla data dropout hard to keep original sequence for training
	- 이러한 문제점을 피하기 위해서 제안 됨
	- 구체적으로 original sequence들 중에서 probability $p_k$ (hyperparameter in (0, 1))로  data dropout을 하지 않는다
	- $1 - p_k$ 확률로 선택된 문장에서 다른 확률 p로 token을 drop

## 3.4 Theoretical Analysis
- Probability p
- Layer representation $h \in \mathbb R^d$
- Randomly sample a scaling vector $\xi \in \mathbb R^d$
$$\xi_i = \begin{cases}
   -1 &\text{with probability p }  \\
   \frac p {1-p} &\text{with probability 1-p }
\end{cases}$$

- Feature dropout : $h_{fd} = (1 + \xi) \odot h$
	- $\odot$ =  element-wised product
	- 1 = $(1, 1, \cdot \cdot \cdot, 1)'$

![2.PNG](/assets/img/unidrop/2.PNG)

## 3.5 UniDrop Integration


![3.png](/assets/img/unidrop/3.PNG)
- Data dropout
	- dropping out some word embeddings
- Feature dropout
	- 각 단어의 representations의 several neurons을 randomly drops
- Layer dropout
	- directly하게 layer 자체를 dropout

# Experiments
## 4.1 Neural Machine Translation

### 4.1.1 Datasets
- IWSLT14
	- En <-> De
	- En <-> Ro
	- En <-> NI
	- En <-> Pt-br
- Pre Process
	- Moses toolkit
	- Byte-pair-encoding

### 4.1.2 Model
- Transformer_iwslt_de_en
	- default dropout = 0.3
	- weight decay = 0.0001
- Feature Dropout = 0.1
- Structure Dropout = 0.1
- Data dropout
	- The sequence keep rate $p_k$ = 0.5
	- Token dropout rate p = 0.2
- Beam size = 5
- Length penalty = 1.0
- Evaluation = tokenized BLEU

### 4.1.3 Results



![4.PNG](/assets/img/unidrop/4.PNG)

- Baselines and models with different dropouts
- FD가 가장 영향적
- 4개의 Dropout (FD-1, 2, 3, 4)이 적용되기 때문


![5.PNG](/assets/img/unidrop/5.PNG)


![6.PNG](/assets/img/unidrop/6.PNG)

## 4.2 Text Classification

### 4.2.1 Datasets
- GLUE tasks
- Typical text classification datasets
	- IMDB
	- Yelp
	- AG
	- TREC
### 4.2.2 Model
- $BERT_{base}$
- $RoBERTa_{base}$
- Feature Dropout = 0.1
- Structure Dropout = 0.1
- Data dropout
	- The sequence keep rate $p_k$ = 0.5
	- Token dropout rate p = 0.1

### 4.2.3 Results

![7.PNG](/assets/img/unidrop/7.PNG)


![8.PNG](/assets/img/unidrop/8.PNG)

## 5.1 Overfitting


![9.PNG](/assets/img/unidrop/9.PNG)


- 각 Dropout 기법이 overfitting에 미치는 영향 확인
	-  Unidrop이 가장 overfitting이 잘 방지됨

## 5.2 Ablation Study


![11.PNG](/assets/img/unidrop/11.PNG)


- FD의 영향에 대해서 조금더 자세히 알아봄
	-  FD는 4개로 구성됨
	-  각각의 FD 모두 결과에 영향 미침
	-  FD3가 멀티헤드 모델에서는 더 영향적

- w/o 2-stage
	-  w/o 2-stage DD는 논문에서 제안한 방법이 아닌 일반적인 방법의 Data dropout을 의미
	- 2-stage 방법이 유효함을 보여줌

## 5.3 Effects of Different Dropout Rates



![10.PNG](/assets/img/unidrop/10.PNG)




