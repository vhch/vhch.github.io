---
layout: post
title: (논문 리뷰) A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation
date: 2023-03-09
<!-- description: jekyll chirpy  -->
math: true
mermaid: true
<!-- featuredImage: null -->
<!-- img: null -->
tags: deeplearning nlp transformer 
categories: nlp translation
<!-- image: -->
<!--   src: /assets/img/Chirpy-title-image.jpg -->
<!--   width: 1000   # in pixels -->
<!--   height: 400   # in pixels -->
<!--   alt: image alternative text -->
---


## 1 Introduction 

### 연구 필요성
massive unlabeld text corpora로 self-supervised training한 Large-scale language models로 pre-trained함. 특정한 task에 적용 하려면 task-specific data에 fine tunning 해야 함. 이때 거대한 model parameters의 수와 제한된 task-specific data로 인해서 fine tunning시 성능 하락과 일반화 능력이 떨어짐

이러한 문제점을 피하기 위해서 adversarial training objectives가 제안됨. 구체적으로 label preserving perturbations은 word embedding layer선에서 수행됨. 이는 input noise에 상관없이 model의 prediction을 일관성있게 함. 이때 추가적인 backward passes가 필요하므로 추가적인 computational and memory overhead가 발생함. 

따라서 간단하면서도 효율적인 data augmentation strategies를 소개함.

### 연구 목표 및 내용
multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013; Clark et al., 2018)에서 영감을 받아서 문장의 일부를 지워서 다양한 perturbed samples을 생성 후 문장들이 model을 통과하면 agreement/consensus를 maximize하려고 함. 이때 관련 정보를 활용하지 못하게 input embedding 단계에서 지움.

조금 더 어렵게 하기 위해서 spanbert의 방식대로 연속적으로 erase 하는 걸 추가함.

생성한 data augmentation들의 distribution 차이를 loss 함수 형태로 볼 수 있는 consistency regularization obejctive를 제안함. 기존에는 cross entropy만 사용 했는데 여기서 추가적으로 Jensen-Shannon Divergence consistency loss도 사용.

이렇게 제안한  data augmentation 방법을 NLU tasks와 machine translation tasks로 평가함.

### 연구 Contribution
1. cut off data augmentation 도입
2. span 방식의 cut off 추가
3. JS divergence consistency loss 추가


## 3 Proposed Approach
* motivation에 대해서 논의 함.
* 간단하지만 효과적인 세 가지의 partial view을 제안함.
* cutoff approach를 text generation으로 확대함

### 3.1 Motivation
data augmentation은 original data를 useful semantic information을 활용하게 할 수 있을거이라 가정함. 이를 식으로 살펴보면 f를 augmented examples x들을 input으로 넣음, f(x)가 일정한 값이 나오게 함. 예시들로 adding Gaussian noise, adversarial training, back-translation이 있는데 실제로 model의 robustness가 증가함. 하지만 이러한 방법들은 additional backward passes가 필요함.

이러한 점을 피하기 위해서 computationally efficient하고 additional models이나 external data sources에 영향을 받지 않는 cut-off을 제안함.

multi-view learning에서 영감을 받음. 식으로 살펴보면 하나의 examples x 에서의 two views를 x1 and x2라고 하고 p1, p2를 각 views의 prediction이라고 한다. 
$$P(p1 \not = p2) \ge max\{P_{err}(p_1), P_{err}(p2)\} \qquad(1)$$
위의 식을 살펴보면 p1의 에러와 p2의 에러가 p1, p2의 불일치보다 작은데 이때 p1, p2의 불일치를 개선하면 p1, p2의 에러를 개선할 수 있음을 의미한다.

문장 전체가 아닌 일부분만 이용해서 consistent predictions하게 하면 일반화 성능과 error rates를 낮출 수 있음을 의미한다.

### 3.2 Constructing Partial Views

Transformer based 모델은 all input tokens을 참조 하므로 input embeddings space에서 cut off 해야 한다고 제안함.

input embedding matrix는 $W \in \mathbb R^{L \times d}$로 표현 가능한데 L은 토큰 수, d는 embedding vector의 차원 수이다.

![1.PNG](/assets/img/cutoff/1.PNG)

위의 이미지처럼 L이나 d 두 차원중 하나를 따라서 cut off하는 방법을 dubbed Cutoff라고 제안함. 이때 cutoff는 선택된 vector of zeros로 바꾼다. 이때 BERT와 같은 경우 word, positional, segment embedding으로 구성 되어 있는데 모든 종류를 0으로 바꾼다. span cutoff는 Spanbert와 동일한 방법을 사용했다.

### 3.3 Incorporating Augmented Samples
original input x 하나로 부터 N개의 cutoff samples을 만든다. 이후 이 samples를 다음과 같은 training objective로 training 한다.


![2.PNG](/assets/img/cutoff/2.PNG)

![3.PNG](/assets/img/cutoff/3.PNG)

$\alpha$, $\beta$는  coefficient weight이다. $L_{ce}$는 cross entrophy loss로 original sample과 cutoff sample에 관해서 각각 해준다. $L_{divergence}$는 앞에서 보았던 cutoff sample들 간의 distribution의 차이를 줄여주는 역할을 한다.
이때 각 sample들 간에 divergence함수를 적용하면 cost가 비약적으로 증가하기 때문에 cutoff의 average에 대해서 divergence함수를 적용해준다.

### 3.4 Extensions to Language Generation
 (Lee et al., 2018)에서 text generation systems는 input noise에 매우 민감하다고 보였음. augmented examples을 추가하면 model's generalization을 증가 시킬거라고 예측함. 이에 대해서 machine translation task에서 살펴봄.
 
### 3.5 Computational Complexity
 다른 adversarial based approaches를 살펴보면 FreeLB, SMART는 둘 다 모두 perturbation directions을 결정하기 위해 추가적인 backward passes가 필요함. 하지만 cutoff 따로 추가적인 backward passes가 필요하지 않아서 효율적임.
 
## 4 Experimental Setup
### 4.1 Datasets
 * Glue
 * WMT14 German-to-English
 * IWSLT14

### 4.2 Training Details
Glue는 Roberta를 testbed로 사용(base, large)
WMT14는 Transformer-base 사용

### 4.3 Baselines
1. adversarial training
	* PGD
	* FreeAT
	* FreeLB
	* ALUM
2. other data augmentation strategies
	* Back translation
	논문의 구현과 공정한 비교를 위해서 back translation에도 consistency training objective를 추가함.
	이때 Back translation은 추가적인 mono lingual data가 필요함.
	
## 5 Experimental Results
* token cutoff
* feature cutoff
* span cutoff

를 GLUE, machine translation tasks에서 적용함

### 5.1 GLUE Benchmark Evaluation
cut off는 adversarial training임과 동시에 data augmentation strategies라고 할 수 있다.

![4.PNG](/assets/img/cutoff/4.PNG)

결과를 살펴보면 cutoff가 뛰어난 성능을 보임을 알 수 있다.
### 5.2 Application to Machine Translation


![5.PNG](/assets/img/cutoff/5.PNG)

machine translation에서는 token cutoff가 가장 성능이 잘 나옴. span cutoff 사용시 인코더와 디코더간의 정보 불일치 발생.



![6.PNG](/assets/img/cutoff/6.PNG)

### 5.3 Ablation Study 
#### 5.3.1 The effect of JS divergence loss
JS divergence consistency loss의 영향을 알기 위해서 $\beta$를 0~3.0 까지 변경 하면서 실험을 진행.
`dataset = MNLI`
$\alpha$는 동일하게 1로 설정.



![7.PNG](/assets/img/cutoff/7.PNG)

table에 나온 것처럼 JS가 효과가 있었고 1.0에서 가장 뛰어난 성능을 나타냄.

#### 5.3.2 The effect of Cutoff ratios
`dataset = MNLI`
cutoff 비율에 따른 결과를 알아봄.
[0.05, 0.1, 0.15, 0.2, 0.3, 0.4]에서 실험 진행.


![8.PNG](/assets/img/cutoff/8.PNG)

Token cutoff = 0.15, Feature Cutoff = 0.2, Span cutoff = 0.1에서 가장 뛰어난 성능을 보임.


