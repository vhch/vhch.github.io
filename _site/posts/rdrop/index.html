<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" data-mode="light">
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks" />
<meta property="og:locale" content="en" />
<meta name="description" content="Abstract Dropout은 deep neural networks을 train 할 때 강력하고 넓은 범위에 regularize 할 수 있는 방법 입니다. 하지만 random 적인 요소로 인해서 train과 inference 과정 사이에 inconsistency가 있을 수 있습니다. 본 논문에서는 regularize dropout을 위한 간단한 consistency training strategy인 R-Drop을 소개 합니다. Dropout을 통해서 생성된 submodels의 output distributions을 consistent하게 합니다. 구체적으로 dropout을 통해서 생성된 두개의 sub models sampled을 양방향 KL-divergence을 통해서 output distributions의 차이를 minimize 합니다. 이론적 분석을 통해서 R-Drop이 inconsistency을 감소시킬 수 있음을 나타냅니다." />
<meta property="og:description" content="Abstract Dropout은 deep neural networks을 train 할 때 강력하고 넓은 범위에 regularize 할 수 있는 방법 입니다. 하지만 random 적인 요소로 인해서 train과 inference 과정 사이에 inconsistency가 있을 수 있습니다. 본 논문에서는 regularize dropout을 위한 간단한 consistency training strategy인 R-Drop을 소개 합니다. Dropout을 통해서 생성된 submodels의 output distributions을 consistent하게 합니다. 구체적으로 dropout을 통해서 생성된 두개의 sub models sampled을 양방향 KL-divergence을 통해서 output distributions의 차이를 minimize 합니다. 이론적 분석을 통해서 R-Drop이 inconsistency을 감소시킬 수 있음을 나타냅니다." />
<link rel="canonical" href="http://localhost:4000/posts/rdrop/" />
<meta property="og:url" content="http://localhost:4000/posts/rdrop/" />
<meta property="og:site_name" content="vhch" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-02-01T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks" />
<meta name="twitter:site" content="@twitter_username" />
<meta name="google-site-verification" content="nqMD8mOUs3m2jRGrMob_LZMp-dbKykPmIITCo3mKRe8" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-01T00:00:00+09:00","datePublished":"2023-02-01T00:00:00+09:00","description":"Abstract Dropout은 deep neural networks을 train 할 때 강력하고 넓은 범위에 regularize 할 수 있는 방법 입니다. 하지만 random 적인 요소로 인해서 train과 inference 과정 사이에 inconsistency가 있을 수 있습니다. 본 논문에서는 regularize dropout을 위한 간단한 consistency training strategy인 R-Drop을 소개 합니다. Dropout을 통해서 생성된 submodels의 output distributions을 consistent하게 합니다. 구체적으로 dropout을 통해서 생성된 두개의 sub models sampled을 양방향 KL-divergence을 통해서 output distributions의 차이를 minimize 합니다. 이론적 분석을 통해서 R-Drop이 inconsistency을 감소시킬 수 있음을 나타냅니다.","headline":"(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/rdrop/"},"url":"http://localhost:4000/posts/rdrop/"}</script>
<!-- End Jekyll SEO tag -->


  <title>(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks | vhch
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="vhch">
<meta name="application-name" content="vhch">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.25.0/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"></a>

    <h1 class="site-title">
      <a href="/">vhch</a>
    </h1>
    <p class="site-subtitle fst-italic mb-0"></p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    

    
      

      
        <a
          href="https://github.com/vhch"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/twitter_username"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['vhch66','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">Home</a>
            </span>

          
        
          
        
          
            
              <span>(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  




<!-- return -->




<article class="px-1">
  <header>
    <h1 data-toc-skip>(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks</h1>

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1675177200"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Feb  1, 2023
</time>

      </span>

      <!-- lastmod date -->
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://github.com/vhch">vhch</a>
            
          </em>
        </span>

        <!-- read time -->
        <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="4066 words"
>
  <em>22 min</em> read</span>

      </div>
      <!-- .d-flex -->
    </div>
    <!-- .post-meta -->
  </header>

  <div class="content">
    <h2 id="abstract"><span class="me-2">Abstract</span><a href="#abstract" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Dropout은 deep neural networks을 train 할 때 강력하고 넓은 범위에  regularize 할 수 있는 방법 입니다. 하지만 random 적인 요소로 인해서 train과 inference 과정 사이에 inconsistency가 있을 수 있습니다. 본 논문에서는 regularize dropout을 위한 간단한 consistency training strategy인 R-Drop을 소개 합니다. Dropout을 통해서 생성된 submodels의 output distributions을 consistent하게 합니다. 구체적으로 dropout을 통해서 생성된 두개의 sub models sampled을 양방향 KL-divergence을 통해서 output distributions의 차이를 minimize 합니다. 이론적 분석을 통해서 R-Drop이 inconsistency을 감소시킬 수 있음을 나타냅니다.</p>

<h2 id="1-introduction"><span class="me-2">1 Introduction</span><a href="#1-introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>최근 deep elarning은 다양한 분야에서 놀랄만한 성공을 거두었습니다. deep neural network을 훈련할 때 regularizaiton techniques은 over fitting을 방지하고, 일반화 성능을 증가시키는데 필요불가결의 존재 입니다. 이때 dropout technique이 가장 넓게 쓰이지만 train할 때만 일정 비율의 hidden units을 dropping 하므로 training과 inference 단계에서 무시할 수 없는 inconsistency과 존재한다. inconsistent한 hidden states에 L2 regularization 하는 방법이 존재하지만 넓게 사용되지 않습니다.</p>

<p>본 논문에서는 dropout에 유발되는 training inconsistency을 regularize하는 간단하면서도 더 효과적인 대안 R-Drop을 제안합니다. 구체적으로 각 mini-batch training에서 각 data sample을 두 번 forward pass 합니다. 각 pass는 randomly dropping out에 의해 다른 sub model distribution이 됩니다. R-Drop은 두 개의 distribution을 bidirectional Kullback-Leibler divergence을 이용해서 minimizing 합니다. 이러한 방식을 사용하면 training과 inference estage의 inconsistency가 완화됩니다. 기존의 training 과정과 비교해보면 R-Drop은 추가적인 구조적인 변경 없이 오직 KL-divergence loss을 추가한 것 뿐입니다.</p>

<p><a href="/assets/img/rdrop/1.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/1.png" alt="1.png" loading="lazy"></a></p>

<p>R-Drop regularization은 간단하지만 매우 효과적임을 광범위한 18개의 datasets 5개의 tasks에서 발견할 수 있었습니다.(30.91 BLEU score on WMT14 English→German and 43.95 on WMT14 English→French translation tasks)</p>

<p>main contribution은 아래와 같이 요약할 수 있습니다.</p>
<ul>
  <li>간단하면서도 효과적인 regularization 방법인 R-Drop을 제안함. 다른 종류의 deep models을 train할 때 universally하게 적용 가능합니다.</li>
  <li>이론적으로 R-Drop이 dropout based models에서 training과 inference시의 inconsistency을 줄일 수 있음을 보여줍니다.</li>
  <li>18개의 dataset에서 4 NLP와 1 CV tasks을 통해서 R-Drop 강력한 성능을 달성할 수 있음을 보입니다.</li>
</ul>

<h2 id="2-approach"><span class="me-2">2 Approach</span><a href="#2-approach" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>R-Drop의 전반적인 구조는 Figure 1에 나와 있습니다. detail한 부분에 대해서 설명하기 앞서 필요한 정보를 설명 합니다.
training dataset D는 아래와 같이 주어집니다. n은 training samples의 수, $(x_i,y_i)$는 labeled data pair, $x_i$는 input data, $y_i$는 label을 의미 합니다.  예시로 machine translation에서 $x_i$는 source language sentence, $y_i$는 target language sentence라고 할 수 있습니다.
$D = {(x_i,y_i)}_{i=1}^n$</p>

<p>$P^w(y|x)$, 이 식은 훈련하고자 하는 모델의 probability distribution(model output)을 의미 합니다. 이후 두 개의 distributions $P_1$, $P_2$ 사이의 KL divergence는 $D_{KL}(P_1||P_2)$로 나타 냅니다.</p>

<h3 id="21-r-drop-regularization"><span class="me-2">2.1 R-Drop Regularization</span><a href="#21-r-drop-regularization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>deep learning model의 main learning 목적은 아래와 같은 negative log-likelihodd loss function을 최소화 하는 것입니다.</p>

\[\begin{align}L_{nll}=\frac{1}{n}\sum_{i=1}^n-logP^w(y_i|x_i)\end{align} \qquad(1)\]

<p>이때 딥러닝 네트워크는 over-fitting 하는 경향이 있는데 이를 줄이기 위해서 dropout과 같은 regularization methods를 채택 합니다. 하지만 dropout을 사용하게 되면 training stage에서는 randomly dropped된 sub model을, inference phase에서는 dropout이 없는 full model을 이용하므로 inconsistency가 발생하게 됩니다. 이를 막기 위해 논문에서는 R-Drop을 제안하고 있습니다.</p>

<p>구체적으로 input data $x_i$를 각 training  step에서 forward pass를 두 번 반복합니다. 그러면 두개의 $P_1^w(y|x)$, $P_2^w(y|x)$ distributions of the model predictions을 얻을 수 있습니다. 
그러면 training step에서 이 두 distribution에 bidirectional Kullback Leibler divergence를 적용해서 minimizing 하려고 합니다.</p>

\[\begin{align}
L^i_{KL}=\frac12(D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i))+D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i))) 
\end{align} \qquad(2)\]

<p>두 개의 forward passes에 대한 기본적인 negative log-likelihood learning objective $L^i_{NLL}$는 아래와 같이 나타낼 수 있습니다.</p>

\[\begin{align}L^i_{NLL}=-logP_1^w(y_i|x_i)-logP_2^w(y_i|x_i)\end{align} \qquad(3)\]

<p>최종 training objective는 $L^i$를 minimize 하는 것입니다.</p>

\[\begin{equation}
\begin{split}
L^i = L^i_{NLL} + \alpha \cdot L^i_{KL} = -logP_1^w(y_i|x_i)-logP_2^w(y_i|x_i)  \\ + \frac{\alpha}2(D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i))+D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i)))
\end{split}
\end{equation}
\qquad (4)\]

<p>$\alpha$는 $L^i_{KL}$을 조정하기 위한 coefficient weight 입니다.</p>

<h3 id="22-training-algorithm"><span class="me-2">2.2 Training Algorithm</span><a href="#22-training-algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p><a href="/assets/img/rdrop/2.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/2.png" alt="2.png" loading="lazy"></a></p>

<p>R-Drop의 전체적인 training algorithm은 위의 Algorithm1에 나와 있습니다. Line 3-4는 model에 forward 해서 $P_1^w(y|x)$, $P_2^w(y|x)$ output distributions을 얻는 것을 나타내고, Line 5-6는 두 distribution 사이에서 NLL 와 KL-divergence를 얻는 것을 나타 냅니다. 
여기서 중요한 것은 forward를 두번 반복하는 것이 아닌 input data x를 복사해서 batch size 차원에서 concatenate해서 training cost를 줄입니다. 마지막으로 Line 7에서 parameters을 식 (4)의 loss를 이용해서 update 합니다.</p>

<h3 id="23-theoretical-analysis"><span class="me-2">2.3 Theoretical Analysis</span><a href="#23-theoretical-analysis" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>R-Drop의 regularization effect에 대해서 분석합니다. $h^l(x) \in \mathbb{R^d}$은 input vector x에 대한 l-th layer의 output을 의미 합니다. $\varepsilon \in \mathbb{R}^d$은 random vector를 의미하며, 각 차원은 독릭적으로 Bernoulli distribution B(p)를 따릅니다.</p>

\[\varepsilon_i^l = 
\begin{cases}
1, \quad with \; probability \; p, \\
   0, \quad with \; probability \; 1-p
\end{cases}\]

<p>$h^l(x)$에 dropout이 적용을 하면 다음과 같이 표현할 수 있습니다. $h^l_{\varepsilon^l}(x) = \frac1p\varepsilon^l \bigodot h^l(x)$ ($\bigodot$은 elemtwise를 의미)
parameter w를 가지고 있는 neural network에 dropout을 적용 한 output distribution은 다음과 같습니다.
$P_\varepsilon^w(y|x):=softmax(linear(h_{\varepsilon^L}^L(\cdot\cdot\cdot h_{\varepsilon^1}^1(x_{\varepsilon^0})))))$, where $\varepsilon = (\varepsilon^L,\cdot\cdot\cdot,\varepsilon^0)$</p>

<p>R-Drop은 다음과 같은 제한된 최적화 공식을 해결하는 것으로 나타낼 수 있습니다.</p>

\[\begin{align}
&amp;\min_w \frac1n\sum_{i=1}^n\mathbb{E_\varepsilon}[-logP_\varepsilon^w(y_i|x_i)], \qquad(5)\\
&amp;s.t. \frac1n \sum_{i=1}^n \mathbb{E}_{\varepsilon^{(1)}, \varepsilon^{(2)}} [D_{KL}(P_{\varepsilon^{(1)}}^w(y_i|x_i)||P_{\varepsilon^{(1)}}^w(y_i|x_i))] \leq\epsilon. \qquad(6)
\end{align}\]

<p>R-Drop optimizes는 식 (5)와 식 (6)를 확률적 방법으로 최적화 하는 방법입니다. 논문에서 제안한 R-Drop은 Bernoulli distribution으로 부터 생성된 두 개의 랜덤 벡터 $\varepsilon^{(1)}$, $\varepsilon^{(2)}$ (twice forward pass를 하면서 작용하는 random dropout을 나타냄)와 training instance $(x_i,y_i)$와 parameter w를 앞에서 제안된 식 (4) $\mathcal L^i$를 parameter w에 대해서 stochastic gradient 하는것을 의미 합니다.($\triangledown_w\mathcal L^i$)</p>

<p>이때 이러한 식이 제안된 이유는  dropout은 training과 inference 사이의 inconsistency 문제가 존재합니다. 
deep learning의 training objective는 sub model들의(각 batch마다 dropout 적용으로 인해 submodels들이 발생) average loss들을 최소화 하는 것입니다(식 (5) 의미)</p>
<blockquote>
  <p>full model은 $\tilde{P}^w(y|x)$로 나타내고, inference에 사용</p>
</blockquote>

<p>거기서 식 (6)의 제약조건을 이용하면 training과 inference 사이의 gap을 줄일 수 있습니다. (자세한 증명은 Appendix B)</p>

<h3 id="24-discussion"><span class="me-2">2.4 Discussion</span><a href="#24-discussion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>R-Drop과 가장 유사한 연구는 ELD와 FD가 있습니다. 그들 또한 consistency training with drop에 관해서 연구하지만, R-Drop은 그들과 다른점이 있습니다.</p>
<ol>
  <li>gap control을 다른 관점에서 합니다. R-Drop과 FD는 sub models 들끼리의 gap을 줄이는데 반해서, ELD는 직접 train과 inference의 gap을 줄입니다. sub models 끼리 gap을 줄이는 것의 장점은 FD에 증명되어 있습니다.</li>
  <li>regularization efficiency가 다릅니다. ELD는 오직 하나의 sub-model에서만 back-propagates를 합니다. 그에 반해서 R-Drop는 두 sub models 모두에서 back-propagates를 합니다.</li>
  <li>regularization effect가 다릅니다. ELD, FD는 L2 distance loss function을 사용합니다. 이것은 main training obejective인 NLL loss와 거리가 있습니다. main objective에는 log-softmax 함수가 적용되는데 이로 인해서 L2 distance 함수로 minimize하면 vector space내의 분포를 정확하게 표현하지 못합니다. KL divergence는 이와 다르게 제대로 표현 가능합니다.(Appendix C.4 참조)</li>
</ol>

<h2 id="3----experiments"><span class="me-2">3    Experiments</span><a href="#3----experiments" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>실험은 5가지 tasks에 관해서 진행함. 4 NLP, 1 CV. neural machine translation (6 datasets), abstractive summarization (1 datasets), language understanding (8 datasets), language modeling (1 dataset), image clasification (2 datasets). 편의를 위해서 R-Drop을 RD로 표기합니다.</p>

<h3 id="31----application-to-neural-machine-translation"><span class="me-2">3.1    Application to Neural Machine Translation</span><a href="#31----application-to-neural-machine-translation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>먼저 NMT tasks에 관해서 평가를 진행 합니다. 실험은 low-resource와 rich-resource에 관해서 진행 합니다.</p>

<h4 id="datasets"><span class="me-2">Datasets</span><a href="#datasets" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>low resource는 IWSLT14 English &lt;-&gt; German, English &lt;-&gt; Spanish, IWSLT17 English &lt;-&gt; French, English &lt;-&gt; Chinese을 사용 하였습니다.
rich resource는 WMT14 English -&gt; German, English -&gt; French을 사용 하였습니다.</p>

<h4 id="model--training"><span class="me-2">Model &amp; Training</span><a href="#model--training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>model은 transformer은 사용 하였습니다. $\alpha$ = 5로 세팅.</p>

<h4 id="results"><span class="me-2">Results</span><a href="#results" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>평가는 BLEU Scores를 사용 하였습니다. IWSLT 성능은 Table 1에, WMT 결과는 Table 2에 나타 냈습니다.</p>

<p><a href="/assets/img/rdrop/3.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/3.png" alt="3.png" loading="lazy"></a></p>

<p><a href="/assets/img/rdrop/4.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/4.png" alt="4.png" loading="lazy"></a></p>

<p>8개의 IWSLT에서 base 보다 2.0 BLEU 향상을 확인할 수 있습니다.  WMT의 score는 기존 SOTA model인 BERT-fused NMT(monolingual data 활용)을 뛰어 넘을 수 있었습니다.</p>

<h3 id="32----application-to-language-understanding"><span class="me-2">3.2    Application to Language Understanding</span><a href="#32----application-to-language-understanding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<h4 id="dataset"><span class="me-2">Dataset</span><a href="#dataset" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>평가는 pre-trained models에 R-Drop을 적용하여 fine-tunning한 모델을 GLUE dataset에서 진행 하였습니다.</p>

<h4 id="model--traning"><span class="me-2">Model &amp; Traning</span><a href="#model--traning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>BERT-base, RoBERTa-large을 pre-trained models로 사용합니다. $\alpha$는 {0.1, 0.5, 1.0} 다양한 세팅으로 진행 하였습니다.</p>

<h4 id="results-1"><span class="me-2">Results</span><a href="#results-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>STS-B는 Pearson correlation, CoLA는 Matthew’s correlation, 나머지 tasks는 Accuracy로 평가를 진행 하였습니다. 결과는 아래 Table 3에 나와 있습니다.</p>

<p><a href="/assets/img/rdrop/5.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/5.png" alt="5.png" loading="lazy"></a></p>

<p>결과를 보면 R-Drop을 사용한 방법은 기존 baselines인 BERT-base, RoBERTa-large에 비해서 1.21 points 와 0.80 points가 상승한 것을 확인할 수 있습니다.</p>

<h3 id="33----application-to-summarization"><span class="me-2">3.3    Application to Summarization</span><a href="#33----application-to-summarization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<h4 id="dataset-1"><span class="me-2">Dataset</span><a href="#dataset-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>CNN/Daily Mail dataset을 사용 하였습니다.</p>
<h4 id="model--training-1"><span class="me-2">Model &amp; Training</span><a href="#model--training-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>pre-trained 된 sequence-to-sequence BART model을 backbone으로 사용 하였고 fine-tune을 R-Drop을 적용해서 진행 하였습니다. $\alpha$ 는 0.7로 설정 하였습니다.</p>
<h4 id="results-2"><span class="me-2">Results</span><a href="#results-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>평가는 ROUGE F1 score로 진행 하였습니다. 실험 결과는 아래 Table 4와 같습니다.</p>

<p><a href="/assets/img/rdrop/6.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/6.png" alt="6.png" loading="lazy"></a></p>

<p>결과 base 모델보다 0.3 points 증가, SOTA를 달성할 수 있었습니다.</p>

<h3 id="34----application-to-language-modeling"><span class="me-2">3.4    Application to Language Modeling</span><a href="#34----application-to-language-modeling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<h4 id="dataset-2"><span class="me-2">Dataset</span><a href="#dataset-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>Dataset은 Wikitext-103 dataset을 사용 하였습니다.</p>

<h4 id="model--training-2"><span class="me-2">Model &amp; Training</span><a href="#model--training-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>model은 두가지를 사용 했는데 하나는 기본적인 Transformer decoder model이고, 다른 하나는 Adaptive Input Transformer(adaptive input embeddings)입니다. $\alpha$는 1.0으로 세팅 하였습니다.</p>

<h4 id="results-3"><span class="me-2">Results</span><a href="#results-3" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>평가 방법은 perplexity를 사용 하였습니다. 결과는 아래의 Table 5와 같습니다.</p>

<p><a href="/assets/img/rdrop/7.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/7.png" alt="7.png" loading="lazy"></a></p>

<p>결과를 보면 base로 사용한 두 모델 모두에서 R-Drop을 적용 시 성능이 향상한 것을 확인할 수 있습니다.</p>

<h3 id="35----application-to-image-classification"><span class="me-2">3.5    Application to Image Classification</span><a href="#35----application-to-image-classification" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<h4 id="dataset-3"><span class="me-2">Dataset</span><a href="#dataset-3" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>이미지 분류를 위한 데이터 셋은 CIFAR-100, ILSVRC-2012 ImageNet dataset을 사용 하였습니다.</p>

<h4 id="model--training-3"><span class="me-2">Model &amp; Training</span><a href="#model--training-3" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>모델은 Vision Transformer(ViT)를 사용 하였습니다. pre-trained model을 fine-tunning 하는 과정에서 R-Drop을 적용 하였습니다. 이때 $\alpha$는 0.6으로 설정 하였습니다.</p>
<h4 id="results-4"><span class="me-2">Results</span><a href="#results-4" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>평가는 Accuracy로 이루어 졌습니다. 결과는 아래 Table 6에 나와 있습니다.</p>

<p><a href="/assets/img/rdrop/8.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/8.png" alt="8.png" loading="lazy"></a></p>

<p>결과를 살펴보면 성능이 증가한 것을 확인할 수 있습니다.</p>

<h2 id="4----study"><span class="me-2">4    Study</span><a href="#4----study" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>이 섹션에서는 R-Drop method에 대해 좀 더 이해하기 위해서 다른 관점에서 광범위한 연구를 진행하였습니다. 분석 실험은 IWSLT 14 De -&gt; En translation task에서 진행 하였습니다.</p>

<h3 id="41----regularization-and-cost-analysis"><span class="me-2">4.1    Regularization and Cost Analysis</span><a href="#41----regularization-and-cost-analysis" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>먼저 R-Drop의 정규화 효과를 보이고 training cost에 대한 잠제적 한계를 살펴 봅니다. training/valid loss, valid BLEU를 training update number와 training time에 따라서 plot 합니다.
plot한 curves는 아래 Figure 2에 나와 있습니다.</p>

<p><a href="/assets/img/rdrop/9.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/9.png" alt="9.png" loading="lazy"></a></p>

<p>우리는 다음과 같은 사실을 관측할 수 있습니다.</p>
<ol>
  <li>training 과정을 살펴보면 transformer가 R-drop에 비해서 빠르게 over-fitting되고, train과 valid loss 간의 gap이 큰 것을 확인할 수 있습니다. 이것은 R-Drop의 regularization이 잘 작동함을 의미 합니다.</li>
  <li>training stage의 초반부에 Transformer는 BLUE score가 빠르게 증가 합니다. 그러나 빠르게 나쁜 최적점으로 수렴 됩니다. 이와 비교해서 R-Drop는 천천히 BLEU가 증가 하지만, 더 뛰어난 성능을 달성합니다. R-Drop이 training cost를 증가 시키지만 이를 무시해도 될 정의 성능증가가 있음이 그림상에서 보여지고 있습니다.</li>
</ol>

<h3 id="42----k-step-r-drop"><span class="me-2">4.2    k-step R-Drop</span><a href="#42----k-step-r-drop" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>기존에는 각 step마다 R-Drop을 적용 하였지만 training efficiency를 높이기 위해서 k-step 마다 R-Drop을 적용 했을때의 효과를 알아 봅니다. 그에 대한 결과는 위의 Figure 3에 나와 있습니다.  결과를 살펴보면 k-step이 높을수록 수렴이 빠른것은 확인할 수 있지만, 더 나쁜 최적점에 도달하는 것을 볼 수 있습니다.</p>

<h3 id="43----m-time-r-drop"><span class="me-2">4.3    m-time R-Drop</span><a href="#43----m-time-r-drop" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>논문에서 제안한 방법은 두 distributions인 $P_1^w(y|x)$ $P_2^w(y|x)$을 regularizes 하는 방법 입니다. 2개대신 m개를 regularizes 하면 성능이 어떻게 될지에 대해서 측정하였습니다. KL divergence에 관한 식은 아래와 같이 수정할 수 있습니다.
\(L_{KL}=\frac{\alpha}{m * (m-1)}\Sigma_{i, j \in 1,\cdot \cdot \cdot, m}^{i \not = j} D_{KL}(P_i^w(y|x)||P_j^w(y|x))\)</p>

<p>이를 이용해서 m=3에 대해서 실험을 진행해 본 결과 37.30을 얻을 수 있었고 m=2 일때는 37.25 였던 결과에 비해서 그렇게 성능 증가폭이 크지 않음을 확인할 수 있습니다. m=2 일때 이미 충분히 강력함을 알 수 있습니다.</p>

<h3 id="44----two-dropout-rates"><span class="me-2">4.4    Two Dropout Rates</span><a href="#44----two-dropout-rates" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>IWSLT translation에서 모델의 dropout은 0.3 입니다. 이때 dropout의 영향에 대해서 알아보기 위해서 {0.1, 0.2, 0.3, 0.4, 0.5}의 dropout 집합에서 2 개씩 선택하여서 실험을 진행 하였습니다. 결과는 아래 Figure 4에 나와 있습니다.</p>

<p><a href="/assets/img/rdrop/10.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/10.png" alt="10.png" loading="lazy"></a></p>

<p>결과를 살펴보면</p>
<ol>
  <li>(0.3, 0.3)에서 가장 뛰어난 성능을 나타 냈습니다.</li>
  <li>(0.3 ~ 0.5)의 합리적인 범위 안에서 큰 성능차이 없이 뛰어난 성능을 보였습니다.</li>
</ol>

<h3 id="45----effect-of-weight"><span class="me-2">4.5    Effect of Weight</span><a href="#45----effect-of-weight" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>KL-divergence loss weight인 $\alpha$에 대해서 조사를 진행 하였습니다. 기존 실험은 $\alpha$ = 5 에 관해서 진행 하였는데, 본 실험은 {1, 3, 5, 7, 10}의 범위에서 선택 하였습니다. 결과는 아래 Table 7에 나와 있습니다.</p>

<p><a href="/assets/img/rdrop/11.png" class="popup img-link  shimmer"><img src="/assets/img/rdrop/11.png" alt="11.png" loading="lazy"></a></p>

<p>결과를 살펴보면 너무 작거나 너무 클 떄 보다 5일때 제일 성능이 좋은것을 확인할 수 있습니다. 이러한 $\alpha$는 각 task에 따라서 최적의 값이 다른데 data size나 model size가 다르기 때문입니다.</p>

<h2 id="5----related-work"><span class="me-2">5    Related Work</span><a href="#5----related-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="regularization-methods"><span class="me-2">Regularization Methods</span><a href="#regularization-methods" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>over fitting을 피하기 위해 많은 regularization이 제안 되었습니다. weight decay, dropout, normalization, adding noise, layer-wise pre-training and initialization, label-smoothing. 이 중 dropout이 평범한 cost로 좋은 성능을 낼 수 있어서 가장 유명합니다.</p>

<h3 id="consistency-training"><span class="me-2">Consistency Training</span><a href="#consistency-training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>앞에서 살펴봤던 ELD, FD, Cutoff에 대해서 이야기 하는데
ELD, FD는 둘다 L2 loss function을 사용 하였고, ELD는 sub model(train 상태)와 inference 모델 간의 consistency만 신경쓰고, FD는 submodels 들간의 consistency를 고려 합니다.
cutoff는 R-Drop과 비슷하게 KL-divergence를 쓰지만, sub models들 간이 아니라 문장에 빈칸을 줘서 빈칸인 sentence와 full sentence간의 consistency training을 진행 합니다.</p>

<h3 id="self-distillation"><span class="me-2">Self-distillation</span><a href="#self-distillation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>두개의 다른 models의 output distributions 간의 KL-divergence를 최소화 하는 것은 knowledge distilation의 student, teacher 방식과 비슷하다고 볼 수 있습니다. 다른 점이라면 R-Drop은 self knowledge distillation, 추가적인 parameter가 필요 없다는 점입니다.</p>

<h2 id="6----conclusions-and-future-work"><span class="me-2">6    Conclusions and Future Work</span><a href="#6----conclusions-and-future-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>본 논문에서는 간단하면서도 효과적인 consistency training method, R-Drop를 제안 하였습니다. 실험결과 R-Drop은 미리 훈련된 강력한 모델뿐 아니라 large-scale datasets으로 훈련 할 때도 잘 작동하였습니다. computational resources의 제한으로 인해 몇몇 tasks는 downstream task에만 R-Drop을 적용 하였지만, pre-training을 후일에 진행할 예정 입니다.</p>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/nlp/">nlp</a>,
          <a href="/categories/translation/">translation</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/deeplearning/"
            class="post-tag no-text-decoration"
          >deeplearning</a>
        
          <a
            href="/tags/nlp/"
            class="post-tag no-text-decoration"
          >nlp</a>
        
          <a
            href="/tags/transformer/"
            class="post-tag no-text-decoration"
          >transformer</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=(%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)%20R-Drop,%20Regularized%20Dropout%20for%20Neural%20Networks%20-%20vhch&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Frdrop%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=(%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)%20R-Drop,%20Regularized%20Dropout%20for%20Neural%20Networks%20-%20vhch&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Frdrop%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Frdrop%2F&text=(%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)%20R-Drop,%20Regularized%20Dropout%20for%20Neural%20Networks%20-%20vhch" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/bisimcut/">(논문 리뷰) Bi-SimCut A Simple Strategy for Boosting Neural Machine Translation</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/unidrop/">(논문 리뷰) UniDrop A Simple yet Effective Technique to Improve Transformer without Extra Cost</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/cutoff/">(논문 리뷰) A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/rdrop/">(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/bibert/">(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deeplearning/">deeplearning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/nlp/">nlp</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/transformer/">transformer</a>
      
    </div>
  </section>


            </div>

            
              
              



  <section id="toc-wrapper" class="ps-0 pe-4">
    <h2 class="panel-heading ps-3 pt-2 mb-2">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->














  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  











  <aside id="related-posts" aria-labelledby="related-label">
    <h3 class="mb-4" id="related-label">Further Reading</h3>
    <nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        <article class="col">
          <a href="/posts/bibert/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1674054000"
  data-df="ll"
  
>
  Jan 19, 2023
</time>

              <h4 class="pt-0 my-2">(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation</h4>
              <div class="text-muted">
                <p>
                  





                  1. Introduction
Pre-trained language models들은 large-scale unlabeled data에서 훈련합니다. ELMo, BERT, XLNET, XLM 등이 존재하고 다양한 NLP tasks에서 뛰어난 성능을 보이고 있습니다. 뛰어난 성능을 보이는 BERT의 pre-trained 모델을 translation mode...
                </p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/cutoff/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1678287600"
  data-df="ll"
  
>
  Mar  9, 2023
</time>

              <h4 class="pt-0 my-2">(논문 리뷰) A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation</h4>
              <div class="text-muted">
                <p>
                  





                  1 Introduction

연구 필요성
massive unlabeld text corpora로 self-supervised training한 Large-scale language models로 pre-trained함. 특정한 task에 적용 하려면 task-specific data에 fine tunning 해야 함. 이때 거대한 model param...
                </p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/bisimcut/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1678719600"
  data-df="ll"
  
>
  Mar 14, 2023
</time>

              <h4 class="pt-0 my-2">(논문 리뷰) Bi-SimCut A Simple Strategy for Boosting Neural Machine Translation</h4>
              <div class="text-muted">
                <p>
                  





                  Abstract
Bi-Simcut은 간단하지만 효과적인  training strategy 입니다. 이것은 두 단계로 구성 됩니다. 1. bidirectional pretraining 2. unidirectional finetuning. 두 단계 모두 original, cutoff sentence paris간의 output distributions의 c...
                </p>
              </div>
            </div>
          </a>
        </article>
      
    </nav>
  </aside>
  <!-- #related-posts -->


            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/posts/bibert/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation</p>
    </a>
  

  
    <a
      href="/posts/cutoff/"
      class="btn btn-outline-primary"
      aria-label="Newer"
    >
      <p>(논문 리뷰) A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation</p>
    </a>
  
</nav>

            
              
              <!--  The comments switcher -->


            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>©
    <time>2024</time>

    
      <a href="https://github.com/vhch">vhch</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deeplearning/">deeplearning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/nlp/">nlp</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/transformer/">transformer</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- JavaScripts -->

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.1/dist/jquery.min.js,npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.10/dayjs.min.js,npm/dayjs@1.11.10/locale/en.min.js,npm/dayjs@1.11.10/plugin/relativeTime.min.js,npm/dayjs@1.11.10/plugin/localizedFormat.min.js,npm/tocbot@4.25.0/dist/tocbot.min.js,npm/mermaid@10.6.1/dist/mermaid.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    
      <!-- mermaid-js loader -->
<script type="text/javascript">
  (function () {
    function updateMermaid(event) {
      if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {
        const mode = event.data.message;

        if (typeof mermaid === 'undefined') {
          return;
        }

        let expectedTheme = mode === ModeToggle.DARK_MODE ? 'dark' : 'default';
        let config = { theme: expectedTheme };

        /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $('.mermaid').each(function () {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr('data-processed');
          $(this).html(svgCode);
        });

        mermaid.initialize(config);
        mermaid.init(undefined, '.mermaid');
      }
    }

    let initTheme = 'default';
    const html = document.documentElement;

    if (
      (html.hasAttribute('data-mode') && html.getAttribute('data-mode') === 'dark') ||
      (!html.hasAttribute('data-mode') && window.matchMedia('(prefers-color-scheme: dark)').matches)
    ) {
      initTheme = 'dark';
    }

    let mermaidConf = {
      theme: initTheme /* <default|dark|forest|neutral> */
    };

    /* Create mermaid tag */
    document.querySelectorAll('pre>code.language-mermaid').forEach((elem) => {
      const svgCode = elem.textContent;
      const backup = elem.parentElement;
      backup.classList.add('unloaded');
      /* create mermaid node */
      let mermaid = document.createElement('pre');
      mermaid.classList.add('mermaid');
      const text = document.createTextNode(svgCode);
      mermaid.appendChild(text);
      backup.after(mermaid);
    });

    mermaid.initialize(mermaidConf);

    window.addEventListener('message', updateMermaid);
  })();
</script>

    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{snippet}</p>  </article>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

