<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" data-mode="light">
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation" />
<meta property="og:locale" content="en" />
<meta name="description" content="1. Introduction Pre-trained language models들은 large-scale unlabeled data에서 훈련합니다. ELMo, BERT, XLNET, XLM 등이 존재하고 다양한 NLP tasks에서 뛰어난 성능을 보이고 있습니다. 뛰어난 성능을 보이는 BERT의 pre-trained 모델을 translation model에 활용하는 몇가지의 예가 존재합니다." />
<meta property="og:description" content="1. Introduction Pre-trained language models들은 large-scale unlabeled data에서 훈련합니다. ELMo, BERT, XLNET, XLM 등이 존재하고 다양한 NLP tasks에서 뛰어난 성능을 보이고 있습니다. 뛰어난 성능을 보이는 BERT의 pre-trained 모델을 translation model에 활용하는 몇가지의 예가 존재합니다." />
<link rel="canonical" href="http://localhost:4000/posts/bibert/" />
<meta property="og:url" content="http://localhost:4000/posts/bibert/" />
<meta property="og:site_name" content="vhch" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-01-19T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation" />
<meta name="twitter:site" content="@twitter_username" />
<meta name="google-site-verification" content="nqMD8mOUs3m2jRGrMob_LZMp-dbKykPmIITCo3mKRe8" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-01-19T00:00:00+09:00","datePublished":"2023-01-19T00:00:00+09:00","description":"1. Introduction Pre-trained language models들은 large-scale unlabeled data에서 훈련합니다. ELMo, BERT, XLNET, XLM 등이 존재하고 다양한 NLP tasks에서 뛰어난 성능을 보이고 있습니다. 뛰어난 성능을 보이는 BERT의 pre-trained 모델을 translation model에 활용하는 몇가지의 예가 존재합니다.","headline":"(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/bibert/"},"url":"http://localhost:4000/posts/bibert/"}</script>
<!-- End Jekyll SEO tag -->


  <title>(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation | vhch
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="vhch">
<meta name="application-name" content="vhch">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.25.0/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"></a>

    <h1 class="site-title">
      <a href="/">vhch</a>
    </h1>
    <p class="site-subtitle fst-italic mb-0"></p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    

    
      

      
        <a
          href="https://github.com/vhch"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/twitter_username"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['vhch66','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">Home</a>
            </span>

          
        
          
        
          
            
              <span>(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  

  
  

  




<!-- return -->




<article class="px-1">
  <header>
    <h1 data-toc-skip>(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation</h1>

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1674054000"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Jan 19, 2023
</time>

      </span>

      <!-- lastmod date -->
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://github.com/vhch">vhch</a>
            
          </em>
        </span>

        <!-- read time -->
        <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="3814 words"
>
  <em>21 min</em> read</span>

      </div>
      <!-- .d-flex -->
    </div>
    <!-- .post-meta -->
  </header>

  <div class="content">
    <h2 id="1-introduction"><span class="me-2">1. Introduction</span><a href="#1-introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Pre-trained language models들은 large-scale unlabeled data에서 훈련합니다. ELMo, BERT, XLNET, XLM 등이 존재하고 다양한 NLP tasks에서 뛰어난 성능을 보이고 있습니다. 뛰어난 성능을 보이는 BERT의 pre-trained 모델을 translation model에 활용하는 몇가지의 예가 존재합니다.</p>

<ol>
  <li>Model의 Encoder 부분을 random하게 초기화 하는 대신 pre trained된 BERT의 parameter로 초기화 하는 방법</li>
  <li>BERT의 output을 encoder의 각 layer에 활용하는 방법</li>
</ol>

<p>이 논문에서는 간단하게 pre trained language model의 output을 NMT 시스템의 input으로 활용 함으로써 IWLST’14, WMT14에서 back translation 없는 상황에서  state-of-the-art를 달성 하였습니다.</p>

<p>또한 다양한 pre-trained langauge model을 평가하고, specialized pre-trained bilingual model을 구현 하였습니다. 추가로 두 가지 개선점을 도입 하였습니다.</p>

<ol>
  <li>stochastic layer selection</li>
  <li>dual-directional training</li>
</ol>

<p>Fig.1에 그에 대한 overview를 나타냅니다.</p>

<p><a href="/assets/img/bibert/1.PNG" class="popup img-link  shimmer"><img src="/assets/img/bibert/1.PNG" alt="1.PNG" loading="lazy"></a></p>

<h2 id="2--contextualized-embeddings-for-nmt"><span class="me-2">2.  Contextualized Embeddings for NMT</span><a href="#2--contextualized-embeddings-for-nmt" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<h3 id="21-method"><span class="me-2">2.1 Method</span><a href="#21-method" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>이 섹션에서는 pre-trained language model의 last layer(contextualized embeddings)을 활용하여 NMT model을 구성할 때의 효과를 조사 합니다.</p>

<p>논문의 basic NMT models은 6 layer의 transformer translation models을 사용합니다. 구체적으로 source sentence를 frozen pre-trained language model에 넣어서 final layer를 NMT encoder의 embedding layer에 활용 합니다.</p>

<p>먼저 처음으로 간단하게 contextualized embeddings을 이용할 때 얼마나 성능이 향상되는지 살펴보고, 다양한 pre-trained language models이 NMT models에 미치는 영향을 알아 봅니다.</p>

<h3 id="22--existing-pre-trained-models"><span class="me-2">2.2  Existing Pre-Trained Models</span><a href="#22--existing-pre-trained-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>먼저 네 개의 유명한 pre-trained models(two monolingual models, two multilingual models)을 NMT모델에 활용한 것을 살펴 봅니다.</p>

<ul>
  <li><strong>ROBERTA</strong> : BERT보다 더 큰 dataset에서 dynamic masking으로 바꾸고, next sentence prediction을 제거한다.</li>
  <li><strong>GottBERT</strong> : 독일어 버전의 RobertA 모델이다.</li>
  <li><strong>MBERT(cased)</strong> : 104개의 언어로 pre-trained된 multilingual Bert이다.</li>
  <li><strong>XLM-R(base)</strong> : transformer-based에서 100개의 언어로 masked language model train한 모델이다.</li>
</ul>

<h3 id="23--how-do-pre-trained-lms-affect-nmt"><span class="me-2">2.3  How Do Pre-Trained LMs Affect NMT?</span><a href="#23--how-do-pre-trained-lms-affect-nmt" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ul>
  <li><strong>Dataset</strong> : low resource인 IWLST14에서 실험을 진행한다. IWLST’ 14 English-German dataset은 160k parallel bilingual sentence pairs로 구성되어 있습니다.</li>
  <li>
    <p><strong>Settings</strong> : 모델은 six layer, FFN dimension size = 1024, attention heads = 4로 구성된 transformer_iwslt_de_en을 사용합니다. embedding dimension은 pretrained language models에 맞추기 위해서 768을 사용한다. Evaluation metric는 일반적으로 사용되는 tokenized  BLEU  (Papineni  et  al.,  2002)을 사용 합니다.</p>
  </li>
  <li>
    <p><strong>Observations</strong> : IWLST’ 14에 대한 결과는 Table 1.에 나와있다.
<br /> <a href="/assets/img/bibert/2.PNG" class="popup img-link  shimmer"><img src="/assets/img/bibert/2.PNG" alt="2.PNG" loading="lazy"></a>
<br /> pre-trained language models을 활용 하려면 NMT model의 vocabulary가 pre-trained models과 동일해야 한다. random은 pre trained의 last layer을 사용하는 대신 NMT의 embedding layer을 random하게 초기화 하는 방식(기존의 방식과 동일)인데, 각 Language model마다 vocabulary가 다르기 때문에 성능 향상이 vocabulary차이 때문이 아님을 보여주기 위해서 random 방식의 실험도 같이 진행한다.decoder vocabulary size는 8K로 고정한다. 이에 대한 detail은 2.5에 나와있다. random 초기화한 결과는 대부분 비슷한 값을 가진다. embedding layer을 contextualized embeddings으로 교체한 결과 GOTTBERT와 ROBERTA의 성능은 눈에 띄게 증가 하였다. 그러나 MBERT, XLM-R은 약간 증가 하거나 오히려 성능이 감소하는 것을 확인할 수 있습니다.</p>
  </li>
  <li><strong>Curse of Multilinguality</strong> : MBERT와 XLM-R 에서 성능이 하락하는 것을 볼 수 있는데 이는 multilinguality 때문인 것으로 보고 있다. low-resource language performance는 higher resource languages를 추가하면 증가합니다. 하지만 higher resource의 성능은 내려가게 된다. English와 German은 higher resource에 속하게 되고 성능이 하락하게 된다. 이러한 원인은 model의 parameter의 capacity가 부족해서 그렇습니다.</li>
</ul>

<h3 id="24--customized-pre-trained-lm"><span class="me-2">2.4  Customized Pre-Trained LM</span><a href="#24--customized-pre-trained-lm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>앞에서 보았듯이 monolingual language models은 machine translation systems의 성능을 증가시키는 것을 확인할 수 있었지만 machine translation는 본질적으로 bilingual task 입니다.  따라서 논문에서는 monolingual data가 아닌 source language와 target language의 mixture data로 train 하는 방법을 가설로 제기 합니다. 이러한 bilingual pre-trained language model을 <strong>BIBERT</strong>라고 제안합니다. 이러한 BIBERT는 ROBERTa 구조를 베이스로 합니다. $BIBERT_{EN-DE}$는 GOTTBERT의 German data와 추가로 146GB English texts를 활용 합니다.</p>

<h3 id="25--vocabulary-size-selection"><span class="me-2">2.5  Vocabulary Size Selection</span><a href="#25--vocabulary-size-selection" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>encoder의 vocabulary size는 pre-trained models에 맞춰서 고정 되어 있지만, decoder의 size는 선택할 수 있습니다. low-resource machine translation에서는 decoder vocabulary size에 따라서 매우 민감합니다.  Gowda and May (2020)는 large grid search를 통해 8K BPE에서 가장 잘 작동함을 밝혔습니다. IWSLT’ 14 (160K parallel sentences) dataset에서도 적합한지 확인하기 위해 decoder vocabulary sizes (8K, 16K, 24K, 32K)에 대해서 확인 합니다.</p>

<p><a href="/assets/img/bibert/3.PNG" class="popup img-link  shimmer"><img src="/assets/img/bibert/3.PNG" alt="3.PNG" loading="lazy"></a>
<br />Figure 2.를 살펴보면 8K에서 가장 뛰어난 BLUE score 달성을 확인할 수 있습니다. 이후 후속 실험은 8K vocabulary sizes로 진행 합니다.</p>

<h3 id="26-bibert-performance"><span class="me-2">2.6 BIBERT Performance</span><a href="#26-bibert-performance" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<ul>
  <li>$BIBERT_{EN-DE}$ <strong>results</strong> : 앞의 Tabel 1.에서의 결과를 살펴보면 논문에서 제안한 모델은 EN -&gt; DE에서 baseline보다 2.12 증가한 29.65 score 달성함을 확인할 수 있습니다. DE -&gt; EN에서는 4.06 증가한 37.58 score를 확인할 수 있습니다. 이때 GOTTBERT와의 차이점은 extra English training data를 추가한 것 뿐이지만 1.3 BLUE point의 증가가 있었습니다.</li>
  <li><strong>Analysis :</strong> BIBERT의 뛰어난 성능에 대해서 분석해 보면, BIBERT의 contextualized embeddings output이 GOTTBERT보다 German information을 풍부하게 갖고 있다 생각할 수 있다. 그리고 extra English data를 가짐으로써 더 나은 assist를 translation model에 제공한다. 또한 ROBERTA 보다 더 적은 English training data를 사용하지만 0.68 BLUE의 성능 향상이 있음을 알 수 있다. BIBERT의 뛰어난 성능에 대해서 부연 설명을 하자면
  <br />
    <ol>
      <li>비슷한 의미를 가지는 두 개의 언어에 대한 aligned embeddings for the tokens을 배울 수 있기 때문이다. 즉 source embeddings은 encoder에 aligned target embeddings에 대한 힌트를 제공할 수 있다.</li>
      <li>overlapping된 En-De sub-word units의 Embedding은 NMT encoder에 bilingual 정보를 활용하여 translation이 가능하게 한다.
<br /><a href="/assets/img/bibert/4.PNG" class="popup img-link  shimmer"><img src="/assets/img/bibert/4.PNG" alt="4.PNG" loading="lazy"></a></li>
    </ol>
  </li>
</ul>

<h2 id="3--layer-selection"><span class="me-2">3.  Layer Selection</span><a href="#3--layer-selection" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>기존에는 pre-trained models의 last layer만 활용 했었는데, BERT의 각 layer는 다른 linguistic information을 capture 합니다. 그래서 더 많은 정보를 활용하기 위해서 last layer만 말고 다른 layer도 select 하는 방법에 대해서 알아 봅니다.</p>

<p>$\chi$를 source language sentences의 집합이라고 나타냅니다. 각 source sentence x $\in$ $\chi$, $H^i_B(x)$는 x를 pre trained language model에 넣었을때 $i_{th}$ layer의 contextualize embeddings을 의미 합니다. 이때 $H^i_B(x)$는 다음과 같은 조건, last layer에서 처음 layer 까지 방향으로 K개의 layer만 고려 합니다.
\(H^i_B(x), \,  \forall i \in [M - K + 1, M]\), K는 hyperparameter, M은 pre-trained language model의 총 layers 수 입니다.</p>

<h3 id="31--stochastic-layer-selection"><span class="me-2">3.1  Stochastic Layer Selection</span><a href="#31--stochastic-layer-selection" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>논문에서는 pre-trained language model의 더 많은 layer의 정보를 활용할 수 있는 stochastic layer selection을 제안함. 구체적으로 기존에는 last layer만 활용 했었는데, 각각의 batch에서 pre trained model의 layer중 랜덤하게  하나의 layer를 선택해서 NMT encoder의 input으로 사용함(Fig 3에 나타남).
<a href="/assets/img/bibert/5.png" class="popup img-link  shimmer"><img src="/assets/img/bibert/5.png" alt="5.png" loading="lazy"></a>
sentence x로 부터 NMT encoder에 들어가는 input embeddings을 $H_E(x)$로 나타내며, 훈련과정 중 다음과 같이 정해집니다.</p>

\[H_E(x) = \int_{i=1}^K\mathbb{1}(\frac {i-1}K &lt; p \leq \frac i K)H_B^{M-i+1}(x) \qquad (1)\]

<p>$\mathbb1$은 indicator function이고 p는 [0, 1] 균등하게 분포하는 random variable 입니다. 즉 train할 때 각 batch에서 p가 [0, 1]의 범위에서 random variable로서 정해지고 indicator function는 특정 범위에 속하는 변수만 1로 나오는 함수로서,  K개의 layer 중 $(\frac {i-1}K &lt; p \leq \frac i K)$에 해당하는 하나의 layer만 선택해서 train이 진행 됩니다. 이후 inference step에서는 training에 사용했던 모든 layer output의 expectation 값을 input embedding으로 사용 합니다.
$\mathbb{E}_{p \sim uniform[0, 1]}[H_E(x)]$, 이 식은 inference 과정을 의미하고 식 1을 다음 식2와 같이 수정할 수 있습니다.</p>

\[H_E(x) = \frac 1 K\sum_{i=1}^K H_B^{M-i+1}(x) \qquad (2)\]

<h3 id="32--experiments-and-results"><span class="me-2">3.2  Experiments and Results</span><a href="#32--experiments-and-results" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Table 1에서 가장 뛰어난 성능을 보였던 $BIBERT_{EN-DE}$을 basis로 하여 후속 연구를 진행 합니다. 실험은 Section 2와 동일하게 IWSLT’ 14 dataset으로 진행합니다.</p>

<p><a href="/assets/img/bibert/6.png" class="popup img-link  shimmer"><img src="/assets/img/bibert/6.png" alt="6.png" loading="lazy"></a></p>

<p>Fig 4는 stochastic layer selection의 영향을 보여줍니다. K의 범위는 2~M으로 진행 하는데 $BIBERT_{EN-DE}$에서 레이어 수가 12개 이므로 M=12 입니다. K=1 이면 last layer만 select 하는것으로 section 2와 동일 하므로 제외 합니다. Fig 4를 살펴보면 모든 경우에서 K=1일때 보다 성능이 증가하는 것을 확인할 수 있습니다. K=8 일때 최대 성능을 가집니다.</p>

<h2 id="4-one-model-dual-directional-translations"><span class="me-2">4. One Model, Dual-Directional Translations</span><a href="#4-one-model-dual-directional-translations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>이 섹션에서는 평범한 one-way translation models과 다르게 dual-directional translation models에 대해서 설명 합니다. model이 En -&gt; De De -&gt; En 양방향으로 translate 할 수 있게 하는것이 목적입니다. 모델의 구조는 Section 3와 동일 합니다. 방법은 source sentence와 target sentence를 encoder, decoder에만 넣지 말고, 반대 방향과 정방향을 한번에 같은 model에서 train하는 것이 요지입니다. 이러한 방법의 motivation은 source, target sentences과 서로 contextualized representations을 향상 시켜줄 것이라고 기대하기 때문입니다. 또한 이러한 방법은 data augmentation 관점으로도 볼 수 있습니다.
이 방법의 장점은 다음 2가지 입니다.</p>
<ol>
  <li>추가적인 bitexts 없이 성능 증가를 얻을 수 있습니다.</li>
  <li>모델 구조를 따로 수정할 필요 없이 data preprocessing의 작은 수정만 필요 합니다.</li>
</ol>

<h3 id="41--dataset-preprocessing"><span class="me-2">4.1  Dataset Preprocessing</span><a href="#41--dataset-preprocessing" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>dataset은 그대로 IWSLT’14 En-&gt;De 입니다. data preprocessing 과정은 아래 Fig 5에 나와 있습니다.
<a href="/assets/img/bibert/7.png" class="popup img-link  shimmer"><img src="/assets/img/bibert/7.png" alt="7.png" loading="lazy"></a></p>

<p>기존 src, tgt을 한 곳에서 concatenation후 shuffle하는 간단한 과정입니다. Decoder에서 joint English-German vocabulary size는 12k를 사용 하였습니다.</p>

<h3 id="42--fine-tuning"><span class="me-2">4.2  Fine-tuning</span><a href="#42--fine-tuning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p><a href="https://aclanthology.org/2021.adaptnlp-1.22/">Xu et al. (2021)</a>에서 초기에 in and out of domain of data로 train후 in domain data로 fine-tuning시 성능이 증가한다는 점에서 영감을 받아서 초기에 Dual direction으로 train 시킨 후 mixed 데이터가 아닌 한 방향으로 fine tuning 한다는 것이 요점입니다.</p>

<h3 id="43--experiments-and-results"><span class="me-2">4.3  Experiments and Results</span><a href="#43--experiments-and-results" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p><a href="/assets/img/bibert/8.png" class="popup img-link  shimmer"><img src="/assets/img/bibert/8.png" alt="8.png" loading="lazy"></a></p>

<p>실험 결과는 위의 Table 3에 나와 있습니다.
기존의 one-way에서 dual-directional, fine-tuning, stochastic layer selection을 추가 할 수록 성능이 증가한다는 것을 나타내고 있습니다.</p>

<h2 id="5--high-resource-scenario"><span class="me-2">5.  High-Resource Scenario</span><a href="#5--high-resource-scenario" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<h3 id="51--dataset-and-training-details"><span class="me-2">5.1  Dataset and Training Details</span><a href="#51--dataset-and-training-details" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>앞의 실험들은 low resource인 IWSLT’ 14 였고 이번 섹션에서는 high resource인 WMT’ 14 English-German dataset에 관해서 실험을 진행 합니다. Model configuration은 big transformer with 4096 FFN dimension and 16 attention heads 입니다. 여기서 $BIBERT_{EN-DE}$과 dimensions을 맞춰주기 위해서 hidden size을 1024 -&gt; 768로 바꿔 줍니다. 그리고 기존에는 low resource라 decoder의 vocabulary size를 8K와 12K를 사용 하였는데 여기서는 52K로 바꿔 줍니다.</p>
<h3 id="52--results"><span class="me-2">5.2  Results</span><a href="#52--results" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p><a href="/assets/img/bibert/9.png" class="popup img-link  shimmer"><img src="/assets/img/bibert/9.png" alt="9.png" loading="lazy"></a>
WMT’ 14에서 제공되는 bitext만 사용한 실험결과는 위의 Table 4에 나와 있습니다. 특별하게도 dual-directional translation training이 앞의 low resource와 다르게 특별한 효과를 발휘하지 못하였습니다. 하나의 가능한 시나리오는 model capacity가 mixed domain data를 handle할 정도로 충분히 크지 못하였을 가능성이 있습니다. 여기서 중요한점은 실험이 기존 모델과 다르게 hidden size를 1024 -&gt; 768로 훨씬 작은 사이즈를 했지만 기존의 결과들보다 우수한 성능을 달성한 점입니다.</p>
<h2 id="6--related-work"><span class="me-2">6.  Related Work</span><a href="#6--related-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="61--pre-trained-embeddings"><span class="me-2">6.1  Pre-Trained Embeddings</span><a href="#61--pre-trained-embeddings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>전통적인 pre-trained embeddings은 word2vec, glove, fastText와 같은 type level로 이용 되었습니다. Peters et al.은 여기서 더 나아가서 pre-trained bidirectional LSTM의 context-aware embeddings output로 발전 시켰습니다. 그 후 attention-based transformer module이 나왔고 이어서 GPT, BERT들은 transformer model의 decoders와 encoders들을 이용해서 downstream tasks에 특화 시켰습니다. 거기서 순수한 English models에서 더 나아가서 CAMEMBERT for French, ARABERT for Arabic, Multilingual representaions MBERT, XLMS 등이 발표 되었고, cross lingual learning에 효과적임을 보였습니다. XLM-R은 대규모에서 cross lingual representation을 learning 시키고, multiple cross lingual benchmarks에서 SOTA를 달성 했습니다.</p>

<h3 id="62--mt-with-context-aware-representations"><span class="me-2">6.2  MT with Context-Aware Representations</span><a href="#62--mt-with-context-aware-representations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Imamura and Sumita는 NMT의 encoder part을 지우고 BERT의 output을 directly하게 decoder에 attention mechanism을 적용 했습니다. 그들은 두 가지 단계의 optimization 과정으로 train 하였습니다. 오직 decoder만 train후 BERT를 fine-tuning 하는 방법입니다.</p>

<p>Clinchant et al.은 NMT model의 embedding layer을 BERT embedding layer로 교체하고, encoder의 parameters를 bert의 parameter로 초기화 합니다. 그러나 여전히 NMT이 기대한 만큼 유연하지 않았습니다.</p>

<p>Rothe et al.는 pre-trained checkpoints(e.g. BERT and GPT)를 12 layer NMT  encoder decoder를 초기화 하는데 이용 하였고, SOTA를 달성할 수 있었습니다. 흥미롭게도 GPT를 이용한 방법은 오히려 random initialized 보다 떨어졌습니다.</p>

<p>비슷하게 Ma et al.도 transformer encoder와 decoder를 XLM-R로 초기화하고, multiple bilingual corpora에서 fine tune 하였습니다.</p>

<p>Zhu et al.의 예비실험은 BERT output을 simple하게 이용한 것이 Bert의 parameter 값으로 초기화 한 것보다 뛰어남을 나타냅니다. 그러나 이 실험에 대해서 제한적으로 부족하게 분석 하였습니다. 그들은 주로 BERT-fuse approach 방법에 대해서 집중 하였습니다. BERT의 output을 NMT encoder, decoder에 extra multi-head attentions 합니다.</p>

<p>Weng et al은 BERT의 last layer만 사용하는 대신 layer aware attention mechanism을 도입하여 compound contextual information을 고려하였습니다. 더 나아가서 그들은 knowledge distillation paradigm을 통해 pre-trained representation을 training process 배우는 것을 제안 하였습니다.</p>

<p>Yaramohammadi et al은 English-Arabic translation task에 앞에서 설명한 개선점들을 모두 참조해서 적용 하였습니다. 그러나 corss lingual information extraction tasks에서 더 도움이 되는것으로 나타났습니다.</p>
<h2 id="7--conclusion"><span class="me-2">7.  Conclusion</span><a href="#7--conclusion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>본 논문에서는 mixed text of the source and target language data를 통해 훈련한 BIBERT를 NMT model에 활용합니다. BIBERT, pre-trained language model의 last layer를 단순하게 이용하는 것이 다른 방식의 활용 방법보다 더 뛰어난 결과를 나타냄을 보였습니다. 더욱이 stochastic layer selection method를 소개하여 성능 향상을 보였습니다. 마지막으로 dual direction 방식이 성능 향상에 도움이 됨을 확인 했습니다.</p>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/nlp/">nlp</a>,
          <a href="/categories/translation/">translation</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/deeplearning/"
            class="post-tag no-text-decoration"
          >deeplearning</a>
        
          <a
            href="/tags/nlp/"
            class="post-tag no-text-decoration"
          >nlp</a>
        
          <a
            href="/tags/transformer/"
            class="post-tag no-text-decoration"
          >transformer</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=(%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)%20BERT,%20MBERT,%20or%20BIBERT?%20A%20Study%20on%20Contextualized%20Embeddings%20for%20Neural%20Machine%20Translation%20-%20vhch&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fbibert%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=(%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)%20BERT,%20MBERT,%20or%20BIBERT?%20A%20Study%20on%20Contextualized%20Embeddings%20for%20Neural%20Machine%20Translation%20-%20vhch&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fbibert%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fbibert%2F&text=(%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)%20BERT,%20MBERT,%20or%20BIBERT?%20A%20Study%20on%20Contextualized%20Embeddings%20for%20Neural%20Machine%20Translation%20-%20vhch" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/bisimcut/">(논문 리뷰) Bi-SimCut A Simple Strategy for Boosting Neural Machine Translation</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/unidrop/">(논문 리뷰) UniDrop A Simple yet Effective Technique to Improve Transformer without Extra Cost</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/cutoff/">(논문 리뷰) A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/rdrop/">(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/bibert/">(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deeplearning/">deeplearning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/nlp/">nlp</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/transformer/">transformer</a>
      
    </div>
  </section>


            </div>

            
              
              



  <section id="toc-wrapper" class="ps-0 pe-4">
    <h2 class="panel-heading ps-3 pt-2 mb-2">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->














  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    










  <aside id="related-posts" aria-labelledby="related-label">
    <h3 class="mb-4" id="related-label">Further Reading</h3>
    <nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        <article class="col">
          <a href="/posts/rdrop/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1675177200"
  data-df="ll"
  
>
  Feb  1, 2023
</time>

              <h4 class="pt-0 my-2">(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks</h4>
              <div class="text-muted">
                <p>
                  





                  Abstract
Dropout은 deep neural networks을 train 할 때 강력하고 넓은 범위에  regularize 할 수 있는 방법 입니다. 하지만 random 적인 요소로 인해서 train과 inference 과정 사이에 inconsistency가 있을 수 있습니다. 본 논문에서는 regularize dropout을 위한 간단한 c...
                </p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/cutoff/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1678287600"
  data-df="ll"
  
>
  Mar  9, 2023
</time>

              <h4 class="pt-0 my-2">(논문 리뷰) A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation</h4>
              <div class="text-muted">
                <p>
                  





                  1 Introduction

연구 필요성
massive unlabeld text corpora로 self-supervised training한 Large-scale language models로 pre-trained함. 특정한 task에 적용 하려면 task-specific data에 fine tunning 해야 함. 이때 거대한 model param...
                </p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/bisimcut/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1678719600"
  data-df="ll"
  
>
  Mar 14, 2023
</time>

              <h4 class="pt-0 my-2">(논문 리뷰) Bi-SimCut A Simple Strategy for Boosting Neural Machine Translation</h4>
              <div class="text-muted">
                <p>
                  





                  Abstract
Bi-Simcut은 간단하지만 효과적인  training strategy 입니다. 이것은 두 단계로 구성 됩니다. 1. bidirectional pretraining 2. unidirectional finetuning. 두 단계 모두 original, cutoff sentence paris간의 output distributions의 c...
                </p>
              </div>
            </div>
          </a>
        </article>
      
    </nav>
  </aside>
  <!-- #related-posts -->


            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <div class="btn btn-outline-primary disabled" aria-label="Older">
      <p>-</p>
    </div>
  

  
    <a
      href="/posts/rdrop/"
      class="btn btn-outline-primary"
      aria-label="Newer"
    >
      <p>(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks</p>
    </a>
  
</nav>

            
              
              <!--  The comments switcher -->


            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>©
    <time>2024</time>

    
      <a href="https://github.com/vhch">vhch</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deeplearning/">deeplearning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/nlp/">nlp</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/transformer/">transformer</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- JavaScripts -->

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.1/dist/jquery.min.js,npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.10/dayjs.min.js,npm/dayjs@1.11.10/locale/en.min.js,npm/dayjs@1.11.10/plugin/relativeTime.min.js,npm/dayjs@1.11.10/plugin/localizedFormat.min.js,npm/tocbot@4.25.0/dist/tocbot.min.js,npm/mermaid@10.6.1/dist/mermaid.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    
      <!-- mermaid-js loader -->
<script type="text/javascript">
  (function () {
    function updateMermaid(event) {
      if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {
        const mode = event.data.message;

        if (typeof mermaid === 'undefined') {
          return;
        }

        let expectedTheme = mode === ModeToggle.DARK_MODE ? 'dark' : 'default';
        let config = { theme: expectedTheme };

        /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $('.mermaid').each(function () {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr('data-processed');
          $(this).html(svgCode);
        });

        mermaid.initialize(config);
        mermaid.init(undefined, '.mermaid');
      }
    }

    let initTheme = 'default';
    const html = document.documentElement;

    if (
      (html.hasAttribute('data-mode') && html.getAttribute('data-mode') === 'dark') ||
      (!html.hasAttribute('data-mode') && window.matchMedia('(prefers-color-scheme: dark)').matches)
    ) {
      initTheme = 'dark';
    }

    let mermaidConf = {
      theme: initTheme /* <default|dark|forest|neutral> */
    };

    /* Create mermaid tag */
    document.querySelectorAll('pre>code.language-mermaid').forEach((elem) => {
      const svgCode = elem.textContent;
      const backup = elem.parentElement;
      backup.classList.add('unloaded');
      /* create mermaid node */
      let mermaid = document.createElement('pre');
      mermaid.classList.add('mermaid');
      const text = document.createTextNode(svgCode);
      mermaid.appendChild(text);
      backup.after(mermaid);
    });

    mermaid.initialize(mermaidConf);

    window.addEventListener('message', updateMermaid);
  })();
</script>

    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{snippet}</p>  </article>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

