[
  
  {
    "title": "(논문 리뷰) UniDrop A Simple yet Effective Technique to Improve Transformer without Extra Cost",
    "url": "/posts/unidrop/",
    "categories": "nlp, translation",
    "tags": "deeplearning, nlp, transformer",
    "date": "2023-03-14 00:00:00 +0900",
    





    
    "snippet": "1 Introduction연구 필요성  Over-parameterization and overfitting          Transformer의 다양한 훈련 전략 나옴      위의 전략들이 효과적임에도 overfitting의 위험성 존재      overfitting을 피하기 위해서 Regularization methods 제안됨          ...",
    "content": "1 Introduction연구 필요성  Over-parameterization and overfitting          Transformer의 다양한 훈련 전략 나옴      위의 전략들이 효과적임에도 overfitting의 위험성 존재      overfitting을 피하기 위해서 Regularization methods 제안됨                  weight decay          data augmentation          dropout          parameter sharing                      다양한 dropout techniques 만으로 SOTA 달성 가능한가?          dropout은 추가적인 costs나 resource가 필요 x      연구 목표 및 내용  Unidrop 제안          feature dropout : 전통적인 dropout      structure dropout : LayerDrop (Fan et al., 2020a)                  layer 계층 통 채로 dropout                    data dropout                  randomly dropping out some tokens in an input sequence                      Evaluation          Neural machine translation with 8 translation datasets      Text classification task with 8 benchmark datasets      연구 Contribution  UniDrop 제안          Three different dropout techniques을 통합함      추가적인 cost나 사전지식 필요 x        이론적으로 증명          세 가지 dropout은 Transformer에서 다른 역활을 하며 overfitting을 방지하고 robustness을 증가시킴                  feature dropout          structure dropout          data dropout                      Transformer with UniDrop이 SOTA 달성          광범위한 실험 결과 제시                  sequence generation          classification task                    3 UniDrop3.1 Feature Dropout  FD-1 (attention dropout)  FD-2 (activation dropout)  FD-3 (query, key, value dropout)  FD-4 (output dropout)3.2 Structure Dropout  Three structure dropouts          LayerDrop      DropHead      HeadMask        DropHead, HeadMask          최근 연구에서 multi-head attention mechanism이 attention head의 작은 부분에 의해 지배 받는다고 알려짐      이를 방지하기 위해 특정 전체 heads를 drop하는 DropHead, HeadMask이 제안됨        LayerDrop          higher-level and coarser-grained structure dropout      Drops some entier layers at training time      directly reduces Transformer model size        LayerDrop을 structure dropout으로 채택3.3 Data Dropout  Data dropout          사전 정의된 확률로 랜덤하게 문장의 some words를 제거        Two-stage data dropout strategy 제안          Directly applying vanilla data dropout hard to keep original sequence for training      이러한 문제점을 피하기 위해서 제안 됨      구체적으로 original sequence들 중에서 probability $p_k$ (hyperparameter in (0, 1))로  data dropout을 하지 않는다      $1 - p_k$ 확률로 선택된 문장에서 다른 확률 p로 token을 drop      3.4 Theoretical Analysis  Probability p  Layer representation $h \\in \\mathbb R^d$      Randomly sample a scaling vector $\\xi \\in \\mathbb R^d$\\(\\xi_i = \\begin{cases} -1 &amp;\\text{with probability p }  \\\\ \\frac p {1-p} &amp;\\text{with probability 1-p }\\end{cases}\\)    Feature dropout : $h_{fd} = (1 + \\xi) \\odot h$          $\\odot$ =  element-wised product      1 = $(1, 1, \\cdot \\cdot \\cdot, 1)’$      3.5 UniDrop Integration  Data dropout          dropping out some word embeddings        Feature dropout          각 단어의 representations의 several neurons을 randomly drops        Layer dropout          directly하게 layer 자체를 dropout      Experiments4.1 Neural Machine Translation4.1.1 Datasets  IWSLT14          En &lt;-&gt; De      En &lt;-&gt; Ro      En &lt;-&gt; NI      En &lt;-&gt; Pt-br        Pre Process          Moses toolkit      Byte-pair-encoding      4.1.2 Model  Transformer_iwslt_de_en          default dropout = 0.3      weight decay = 0.0001        Feature Dropout = 0.1  Structure Dropout = 0.1  Data dropout          The sequence keep rate $p_k$ = 0.5      Token dropout rate p = 0.2        Beam size = 5  Length penalty = 1.0  Evaluation = tokenized BLEU4.1.3 Results  Baselines and models with different dropouts  FD가 가장 영향적  4개의 Dropout (FD-1, 2, 3, 4)이 적용되기 때문4.2 Text Classification4.2.1 Datasets  GLUE tasks  Typical text classification datasets          IMDB      Yelp      AG      TREC        4.2.2 Model              $BERT_{base}$  $RoBERTa_{base}$  Feature Dropout = 0.1  Structure Dropout = 0.1  Data dropout          The sequence keep rate $p_k$ = 0.5      Token dropout rate p = 0.1      4.2.3 Results5.1 Overfitting  각 Dropout 기법이 overfitting에 미치는 영향 확인          Unidrop이 가장 overfitting이 잘 방지됨      5.2 Ablation Study  FD의 영향에 대해서 조금더 자세히 알아봄          FD는 4개로 구성됨      각각의 FD 모두 결과에 영향 미침      FD3가 멀티헤드 모델에서는 더 영향적        w/o 2-stage          w/o 2-stage DD는 논문에서 제안한 방법이 아닌 일반적인 방법의 Data dropout을 의미      2-stage 방법이 유효함을 보여줌      5.3 Effects of Different Dropout Rates"
  },
  
  {
    "title": "(논문 리뷰) Bi-SimCut A Simple Strategy for Boosting Neural Machine Translation",
    "url": "/posts/bisimcut/",
    "categories": "nlp, translation",
    "tags": "deeplearning, nlp, transformer",
    "date": "2023-03-14 00:00:00 +0900",
    





    
    "snippet": "AbstractBi-Simcut은 간단하지만 효과적인  training strategy 입니다. 이것은 두 단계로 구성 됩니다. 1. bidirectional pretraining 2. unidirectional finetuning. 두 단계 모두 original, cutoff sentence paris간의 output distributions의 co...",
    "content": "AbstractBi-Simcut은 간단하지만 효과적인  training strategy 입니다. 이것은 두 단계로 구성 됩니다. 1. bidirectional pretraining 2. unidirectional finetuning. 두 단계 모두 original, cutoff sentence paris간의 output distributions의 consistency를 유지하게 하는 정규화 방법 SimCut을 적용 합니다. 이떄 back-translation으로 생성한 extra dataset이나 large-scale pretrained model은 사용하지 않고, Bi-SumCut은 강력한 translation performacne를 달성 하였습니다. Sim-Cut은 새로운 방법이 아니고, Cut-off (Shen et al. 2020)를 간단화하고 NMT에 적용한 방법 입니다.1 Introduction연구 필요성  기존 연구 simcut은 뛰어난 성능을 가짐  하지만 연구에서 소개된 Hyperparameter 4개의 적절한 값을 찾는것은 지루하고 많은 시간이 소모됨.연구 목표 및 내용  NMT 교육을 위한 전략을 제공          간단하고 재현하기 쉽지만 효과적      cutoff 참조      virtual adversarial regularization 참조        SimCut 제안          새로운 방법 x      Cutoff에서 제안된 Token Cutoff를 간소화 함      bidirectional backpropagation in KL regularization이 key role임        Bi-SimCut 제안          SimCut은 perturbation based이면서 augmentation 방법임      두 단계로 training strategy가 구성됨                  bidirectional pretraining with SimCut          unidirectional finetuning with SimCut                    연구 Contribution  SimCut 제안          perturbation based method로 간주 가능      pretrained language models mBART에 대한 호환성을 보임        Bi-SimCut 제안          bidirectional pretraining, unidirectional finetuing with SimCut으로 구성        5가지 translation benchmark에서 인상적인 개선          Transforemr model with Bi-SimCut을 이용      3 Datasets and Baseline SettingsDatasets  IWSLT14 en &lt;-&gt; de (low-resource)  WMT17 zh -&gt; en  (high-resource)Settings  Base model = Transformer  label smoothing = 0.1  max tokens per batch = 4096  Adam optimizer with Beta (0.9, 0.98)  4000 warmup steps  learning rate = 5e-4  dropout rate = 0.3  beam size = 5  length penalty 1.04 Bi-SimCut4.1 SimCut: A Simple Cutoff Regularization for NMT  기존 cut off는 ($p_{cut}$, $\\alpha$, $\\beta$, N)의 parameter를 일일히 찾아야 함  이러한 부담을 줄이기 위해서 SimCut 제안  problem formulation는 Virtual Adversarial Training (VAT)에서 영감을 얻음          KL-based adversarial regularization      adversarial perturbations $\\delta_x \\in \\mathbb R^{d \\times I}$, $\\delta_y \\in \\mathbb R^{d \\times J}$을 original sampels에 더한 것과 original samples의 차이를 줄임      $KL(f(e(x), e(y); \\theta , || f(e(x) + \\delta_x, e(y) + \\delta_y; \\theta))$        Token Cutoff로 perturbation 생성          기존 VAT는 gradient 방식으로 perturbation 생성      각 sentence pair (x, y)에서 하나의 cutoff smaple ($x_{cut}$, $y_{cut}$) 생성      Training objective of Simcut                  $L_{simcut}(\\theta) = L_{ce}(\\theta) + \\alpha L_{simkl}(\\theta)$          $L_{simkl}(\\theta)$ =  $KL(f(x, y; \\theta \\, || f(x_{cut}, y_{cut} ; \\theta))$          두개의 hyper-parameters $\\alpha$, $p_{cut}$만 존재          VAT는 KL의 오른쪽항에 대해서만 back propagation, BiSimCut은 왼, 오른쪽 모두 back propagation                    4.2 Analysis on SimCut4.2.1 How Does the Simplification Affect Performance?  기존 방법보다 뛰어난 성능 달성  training cost에 따른 성능 비교          VAT over fitting 발생      SimCut은 much more training time to converge이지만 over fitting 덜 발생      Token Cutoff costs about 148 seconds per epoch (N=1)      Simcut costs about 128 seconds per epoch      4.2.2 How Does the Bidirectional Backpropagation Affect Performance?  KL 항에서 양쪽항을 back propagation 할 떄의 효과  Bi-backpropagation시 성능 증가4.2.3 Performance on Perturbed Inputs  Simcut은 perturbation method로 간주 가능  기존의 perturbation 방법과 비교  확률 p ~ (0.00, 0.01, 0.05, 0.10)로 replace or drop된 문장으로 실험 진행  SimCut이 perturbed test set에서 가장 robustness 함4.2.4 Effects of $\\alpha$ and $p_{cut}$  $\\alpha \\in {1, 2, 3, 4, 5}$  $p_{cut} \\in {0.00, 0.05, 0.10, 0.15, 0.20}$4.2.5  Is SimCut Compatible with the Pretrained Language Model?  Backbone model : mBART  dataset          IWSLT14 de-&gt;en      only remove the duplicated sentence pairs following mBART50        Transformer          randonly initialized      trained from scratch      configurations in Section 3        mBART          directly finetuned from mBART        mBART with SimCut          finetuned from mBART with SimCut      4.3 Training Strategy : Bidirectional Pretrain and Unidriectional Finetune  English -&gt; German dataset 존재  English + German -&gt; German + English로 먼저 pre train  이후 English -&gt; German으로 fine tunning5 Standard Resource Scenario5.1 Dataset Description and Model Configuration  Dataset : WMT14 English-German (4.5M parallel sentence)  Validation set : newstest2012, newstest2013  Test set : newstest2014  shard dictionary with 52K bpe  Transformer Big model  cross entropy loss with label smoothing rate 0.1  max tokens per batch to be 4096  Adam optimizer with Beta (0.9, 0.98)  4000 warmup steps  inverse square root learning rate scheduler with initial learning rates 1e-3  decrease the learning rate to 5e-4  dropout = [0.3, 0.2, 0.1]  beam size = 4  length penalty 0.65.2 Results  $p_{cut}$ = 0.056 High Resource Scenario6.1 Dataset Description and Model Configuration  Dataset : WMT17 Chinese-English dataset (20.2M training sentence pairs)  Validation set : newsdev2017  Test set : newstest2017  32K BPE vocabularies  Transformer Big model6.2 Results"
  },
  
  {
    "title": "(논문 리뷰) A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation",
    "url": "/posts/cutoff/",
    "categories": "nlp, translation",
    "tags": "deeplearning, nlp, transformer",
    "date": "2023-03-09 00:00:00 +0900",
    





    
    "snippet": "1 Introduction연구 필요성massive unlabeld text corpora로 self-supervised training한 Large-scale language models로 pre-trained함. 특정한 task에 적용 하려면 task-specific data에 fine tunning 해야 함. 이때 거대한 model paramete...",
    "content": "1 Introduction연구 필요성massive unlabeld text corpora로 self-supervised training한 Large-scale language models로 pre-trained함. 특정한 task에 적용 하려면 task-specific data에 fine tunning 해야 함. 이때 거대한 model parameters의 수와 제한된 task-specific data로 인해서 fine tunning시 성능 하락과 일반화 능력이 떨어짐이러한 문제점을 피하기 위해서 adversarial training objectives가 제안됨. 구체적으로 label preserving perturbations은 word embedding layer선에서 수행됨. 이는 input noise에 상관없이 model의 prediction을 일관성있게 함. 이때 추가적인 backward passes가 필요하므로 추가적인 computational and memory overhead가 발생함.따라서 간단하면서도 효율적인 data augmentation strategies를 소개함.연구 목표 및 내용multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013; Clark et al., 2018)에서 영감을 받아서 문장의 일부를 지워서 다양한 perturbed samples을 생성 후 문장들이 model을 통과하면 agreement/consensus를 maximize하려고 함. 이때 관련 정보를 활용하지 못하게 input embedding 단계에서 지움.조금 더 어렵게 하기 위해서 spanbert의 방식대로 연속적으로 erase 하는 걸 추가함.생성한 data augmentation들의 distribution 차이를 loss 함수 형태로 볼 수 있는 consistency regularization obejctive를 제안함. 기존에는 cross entropy만 사용 했는데 여기서 추가적으로 Jensen-Shannon Divergence consistency loss도 사용.이렇게 제안한  data augmentation 방법을 NLU tasks와 machine translation tasks로 평가함.연구 Contribution  cut off data augmentation 도입  span 방식의 cut off 추가  JS divergence consistency loss 추가3 Proposed Approach  motivation에 대해서 논의 함.  간단하지만 효과적인 세 가지의 partial view을 제안함.  cutoff approach를 text generation으로 확대함3.1 Motivationdata augmentation은 original data를 useful semantic information을 활용하게 할 수 있을거이라 가정함. 이를 식으로 살펴보면 f를 augmented examples x들을 input으로 넣음, f(x)가 일정한 값이 나오게 함. 예시들로 adding Gaussian noise, adversarial training, back-translation이 있는데 실제로 model의 robustness가 증가함. 하지만 이러한 방법들은 additional backward passes가 필요함.이러한 점을 피하기 위해서 computationally efficient하고 additional models이나 external data sources에 영향을 받지 않는 cut-off을 제안함.multi-view learning에서 영감을 받음. 식으로 살펴보면 하나의 examples x 에서의 two views를 x1 and x2라고 하고 p1, p2를 각 views의 prediction이라고 한다. \\(P(p1 \\not = p2) \\ge max\\{P_{err}(p_1), P_{err}(p2)\\} \\qquad(1)\\)위의 식을 살펴보면 p1의 에러와 p2의 에러가 p1, p2의 불일치보다 작은데 이때 p1, p2의 불일치를 개선하면 p1, p2의 에러를 개선할 수 있음을 의미한다.문장 전체가 아닌 일부분만 이용해서 consistent predictions하게 하면 일반화 성능과 error rates를 낮출 수 있음을 의미한다.3.2 Constructing Partial ViewsTransformer based 모델은 all input tokens을 참조 하므로 input embeddings space에서 cut off 해야 한다고 제안함.input embedding matrix는 $W \\in \\mathbb R^{L \\times d}$로 표현 가능한데 L은 토큰 수, d는 embedding vector의 차원 수이다.위의 이미지처럼 L이나 d 두 차원중 하나를 따라서 cut off하는 방법을 dubbed Cutoff라고 제안함. 이때 cutoff는 선택된 vector of zeros로 바꾼다. 이때 BERT와 같은 경우 word, positional, segment embedding으로 구성 되어 있는데 모든 종류를 0으로 바꾼다. span cutoff는 Spanbert와 동일한 방법을 사용했다.3.3 Incorporating Augmented Samplesoriginal input x 하나로 부터 N개의 cutoff samples을 만든다. 이후 이 samples를 다음과 같은 training objective로 training 한다.$\\alpha$, $\\beta$는  coefficient weight이다. $L_{ce}$는 cross entrophy loss로 original sample과 cutoff sample에 관해서 각각 해준다. $L_{divergence}$는 앞에서 보았던 cutoff sample들 간의 distribution의 차이를 줄여주는 역할을 한다.이때 각 sample들 간에 divergence함수를 적용하면 cost가 비약적으로 증가하기 때문에 cutoff의 average에 대해서 divergence함수를 적용해준다.3.4 Extensions to Language Generation(Lee et al., 2018)에서 text generation systems는 input noise에 매우 민감하다고 보였음. augmented examples을 추가하면 model’s generalization을 증가 시킬거라고 예측함. 이에 대해서 machine translation task에서 살펴봄.3.5 Computational Complexity다른 adversarial based approaches를 살펴보면 FreeLB, SMART는 둘 다 모두 perturbation directions을 결정하기 위해 추가적인 backward passes가 필요함. 하지만 cutoff 따로 추가적인 backward passes가 필요하지 않아서 효율적임.4 Experimental Setup4.1 Datasets  Glue  WMT14 German-to-English  IWSLT144.2 Training DetailsGlue는 Roberta를 testbed로 사용(base, large)WMT14는 Transformer-base 사용4.3 Baselines  adversarial training          PGD      FreeAT      FreeLB      ALUM        other data augmentation strategies          Back translation 논문의 구현과 공정한 비교를 위해서 back translation에도 consistency training objective를 추가함. 이때 Back translation은 추가적인 mono lingual data가 필요함.      5 Experimental Results  token cutoff  feature cutoff  span cutoff를 GLUE, machine translation tasks에서 적용함5.1 GLUE Benchmark Evaluationcut off는 adversarial training임과 동시에 data augmentation strategies라고 할 수 있다.결과를 살펴보면 cutoff가 뛰어난 성능을 보임을 알 수 있다.5.2 Application to Machine Translationmachine translation에서는 token cutoff가 가장 성능이 잘 나옴. span cutoff 사용시 인코더와 디코더간의 정보 불일치 발생.5.3 Ablation Study5.3.1 The effect of JS divergence lossJS divergence consistency loss의 영향을 알기 위해서 $\\beta$를 0~3.0 까지 변경 하면서 실험을 진행.dataset = MNLI$\\alpha$는 동일하게 1로 설정.table에 나온 것처럼 JS가 효과가 있었고 1.0에서 가장 뛰어난 성능을 나타냄.5.3.2 The effect of Cutoff ratiosdataset = MNLIcutoff 비율에 따른 결과를 알아봄.[0.05, 0.1, 0.15, 0.2, 0.3, 0.4]에서 실험 진행.Token cutoff = 0.15, Feature Cutoff = 0.2, Span cutoff = 0.1에서 가장 뛰어난 성능을 보임."
  },
  
  {
    "title": "(논문 리뷰) R-Drop, Regularized Dropout for Neural Networks",
    "url": "/posts/rdrop/",
    "categories": "nlp, translation",
    "tags": "deeplearning, nlp, transformer",
    "date": "2023-02-01 00:00:00 +0900",
    





    
    "snippet": "AbstractDropout은 deep neural networks을 train 할 때 강력하고 넓은 범위에  regularize 할 수 있는 방법 입니다. 하지만 random 적인 요소로 인해서 train과 inference 과정 사이에 inconsistency가 있을 수 있습니다. 본 논문에서는 regularize dropout을 위한 간단한 co...",
    "content": "AbstractDropout은 deep neural networks을 train 할 때 강력하고 넓은 범위에  regularize 할 수 있는 방법 입니다. 하지만 random 적인 요소로 인해서 train과 inference 과정 사이에 inconsistency가 있을 수 있습니다. 본 논문에서는 regularize dropout을 위한 간단한 consistency training strategy인 R-Drop을 소개 합니다. Dropout을 통해서 생성된 submodels의 output distributions을 consistent하게 합니다. 구체적으로 dropout을 통해서 생성된 두개의 sub models sampled을 양방향 KL-divergence을 통해서 output distributions의 차이를 minimize 합니다. 이론적 분석을 통해서 R-Drop이 inconsistency을 감소시킬 수 있음을 나타냅니다.1 Introduction최근 deep elarning은 다양한 분야에서 놀랄만한 성공을 거두었습니다. deep neural network을 훈련할 때 regularizaiton techniques은 over fitting을 방지하고, 일반화 성능을 증가시키는데 필요불가결의 존재 입니다. 이때 dropout technique이 가장 넓게 쓰이지만 train할 때만 일정 비율의 hidden units을 dropping 하므로 training과 inference 단계에서 무시할 수 없는 inconsistency과 존재한다. inconsistent한 hidden states에 L2 regularization 하는 방법이 존재하지만 넓게 사용되지 않습니다.본 논문에서는 dropout에 유발되는 training inconsistency을 regularize하는 간단하면서도 더 효과적인 대안 R-Drop을 제안합니다. 구체적으로 각 mini-batch training에서 각 data sample을 두 번 forward pass 합니다. 각 pass는 randomly dropping out에 의해 다른 sub model distribution이 됩니다. R-Drop은 두 개의 distribution을 bidirectional Kullback-Leibler divergence을 이용해서 minimizing 합니다. 이러한 방식을 사용하면 training과 inference estage의 inconsistency가 완화됩니다. 기존의 training 과정과 비교해보면 R-Drop은 추가적인 구조적인 변경 없이 오직 KL-divergence loss을 추가한 것 뿐입니다.R-Drop regularization은 간단하지만 매우 효과적임을 광범위한 18개의 datasets 5개의 tasks에서 발견할 수 있었습니다.(30.91 BLEU score on WMT14 English→German and 43.95 on WMT14 English→French translation tasks)main contribution은 아래와 같이 요약할 수 있습니다.  간단하면서도 효과적인 regularization 방법인 R-Drop을 제안함. 다른 종류의 deep models을 train할 때 universally하게 적용 가능합니다.  이론적으로 R-Drop이 dropout based models에서 training과 inference시의 inconsistency을 줄일 수 있음을 보여줍니다.  18개의 dataset에서 4 NLP와 1 CV tasks을 통해서 R-Drop 강력한 성능을 달성할 수 있음을 보입니다.2 ApproachR-Drop의 전반적인 구조는 Figure 1에 나와 있습니다. detail한 부분에 대해서 설명하기 앞서 필요한 정보를 설명 합니다.training dataset D는 아래와 같이 주어집니다. n은 training samples의 수, $(x_i,y_i)$는 labeled data pair, $x_i$는 input data, $y_i$는 label을 의미 합니다.  예시로 machine translation에서 $x_i$는 source language sentence, $y_i$는 target language sentence라고 할 수 있습니다.$D = {(x_i,y_i)}_{i=1}^n$$P^w(y|x)$, 이 식은 훈련하고자 하는 모델의 probability distribution(model output)을 의미 합니다. 이후 두 개의 distributions $P_1$, $P_2$ 사이의 KL divergence는 $D_{KL}(P_1||P_2)$로 나타 냅니다.2.1 R-Drop Regularizationdeep learning model의 main learning 목적은 아래와 같은 negative log-likelihodd loss function을 최소화 하는 것입니다.\\[\\begin{align}L_{nll}=\\frac{1}{n}\\sum_{i=1}^n-logP^w(y_i|x_i)\\end{align} \\qquad(1)\\]이때 딥러닝 네트워크는 over-fitting 하는 경향이 있는데 이를 줄이기 위해서 dropout과 같은 regularization methods를 채택 합니다. 하지만 dropout을 사용하게 되면 training stage에서는 randomly dropped된 sub model을, inference phase에서는 dropout이 없는 full model을 이용하므로 inconsistency가 발생하게 됩니다. 이를 막기 위해 논문에서는 R-Drop을 제안하고 있습니다.구체적으로 input data $x_i$를 각 training  step에서 forward pass를 두 번 반복합니다. 그러면 두개의 $P_1^w(y|x)$, $P_2^w(y|x)$ distributions of the model predictions을 얻을 수 있습니다. 그러면 training step에서 이 두 distribution에 bidirectional Kullback Leibler divergence를 적용해서 minimizing 하려고 합니다.\\[\\begin{align}L^i_{KL}=\\frac12(D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i))+D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i))) \\end{align} \\qquad(2)\\]두 개의 forward passes에 대한 기본적인 negative log-likelihood learning objective $L^i_{NLL}$는 아래와 같이 나타낼 수 있습니다.\\[\\begin{align}L^i_{NLL}=-logP_1^w(y_i|x_i)-logP_2^w(y_i|x_i)\\end{align} \\qquad(3)\\]최종 training objective는 $L^i$를 minimize 하는 것입니다.\\[\\begin{equation}\\begin{split}L^i = L^i_{NLL} + \\alpha \\cdot L^i_{KL} = -logP_1^w(y_i|x_i)-logP_2^w(y_i|x_i)  \\\\ + \\frac{\\alpha}2(D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i))+D_{KL}(P_1^w(y_i|x_i)||P_2^w(y_i|x_i)))\\end{split}\\end{equation}\\qquad (4)\\]$\\alpha$는 $L^i_{KL}$을 조정하기 위한 coefficient weight 입니다.2.2 Training AlgorithmR-Drop의 전체적인 training algorithm은 위의 Algorithm1에 나와 있습니다. Line 3-4는 model에 forward 해서 $P_1^w(y|x)$, $P_2^w(y|x)$ output distributions을 얻는 것을 나타내고, Line 5-6는 두 distribution 사이에서 NLL 와 KL-divergence를 얻는 것을 나타 냅니다. 여기서 중요한 것은 forward를 두번 반복하는 것이 아닌 input data x를 복사해서 batch size 차원에서 concatenate해서 training cost를 줄입니다. 마지막으로 Line 7에서 parameters을 식 (4)의 loss를 이용해서 update 합니다.2.3 Theoretical AnalysisR-Drop의 regularization effect에 대해서 분석합니다. $h^l(x) \\in \\mathbb{R^d}$은 input vector x에 대한 l-th layer의 output을 의미 합니다. $\\varepsilon \\in \\mathbb{R}^d$은 random vector를 의미하며, 각 차원은 독릭적으로 Bernoulli distribution B(p)를 따릅니다.\\[\\varepsilon_i^l = \\begin{cases}1, \\quad with \\; probability \\; p, \\\\   0, \\quad with \\; probability \\; 1-p\\end{cases}\\]$h^l(x)$에 dropout이 적용을 하면 다음과 같이 표현할 수 있습니다. $h^l_{\\varepsilon^l}(x) = \\frac1p\\varepsilon^l \\bigodot h^l(x)$ ($\\bigodot$은 elemtwise를 의미)parameter w를 가지고 있는 neural network에 dropout을 적용 한 output distribution은 다음과 같습니다.$P_\\varepsilon^w(y|x):=softmax(linear(h_{\\varepsilon^L}^L(\\cdot\\cdot\\cdot h_{\\varepsilon^1}^1(x_{\\varepsilon^0})))))$, where $\\varepsilon = (\\varepsilon^L,\\cdot\\cdot\\cdot,\\varepsilon^0)$R-Drop은 다음과 같은 제한된 최적화 공식을 해결하는 것으로 나타낼 수 있습니다.\\[\\begin{align}&amp;\\min_w \\frac1n\\sum_{i=1}^n\\mathbb{E_\\varepsilon}[-logP_\\varepsilon^w(y_i|x_i)], \\qquad(5)\\\\&amp;s.t. \\frac1n \\sum_{i=1}^n \\mathbb{E}_{\\varepsilon^{(1)}, \\varepsilon^{(2)}} [D_{KL}(P_{\\varepsilon^{(1)}}^w(y_i|x_i)||P_{\\varepsilon^{(1)}}^w(y_i|x_i))] \\leq\\epsilon. \\qquad(6)\\end{align}\\]R-Drop optimizes는 식 (5)와 식 (6)를 확률적 방법으로 최적화 하는 방법입니다. 논문에서 제안한 R-Drop은 Bernoulli distribution으로 부터 생성된 두 개의 랜덤 벡터 $\\varepsilon^{(1)}$, $\\varepsilon^{(2)}$ (twice forward pass를 하면서 작용하는 random dropout을 나타냄)와 training instance $(x_i,y_i)$와 parameter w를 앞에서 제안된 식 (4) $\\mathcal L^i$를 parameter w에 대해서 stochastic gradient 하는것을 의미 합니다.($\\triangledown_w\\mathcal L^i$)이때 이러한 식이 제안된 이유는  dropout은 training과 inference 사이의 inconsistency 문제가 존재합니다. deep learning의 training objective는 sub model들의(각 batch마다 dropout 적용으로 인해 submodels들이 발생) average loss들을 최소화 하는 것입니다(식 (5) 의미)  full model은 $\\tilde{P}^w(y|x)$로 나타내고, inference에 사용거기서 식 (6)의 제약조건을 이용하면 training과 inference 사이의 gap을 줄일 수 있습니다. (자세한 증명은 Appendix B)2.4 DiscussionR-Drop과 가장 유사한 연구는 ELD와 FD가 있습니다. 그들 또한 consistency training with drop에 관해서 연구하지만, R-Drop은 그들과 다른점이 있습니다.  gap control을 다른 관점에서 합니다. R-Drop과 FD는 sub models 들끼리의 gap을 줄이는데 반해서, ELD는 직접 train과 inference의 gap을 줄입니다. sub models 끼리 gap을 줄이는 것의 장점은 FD에 증명되어 있습니다.  regularization efficiency가 다릅니다. ELD는 오직 하나의 sub-model에서만 back-propagates를 합니다. 그에 반해서 R-Drop는 두 sub models 모두에서 back-propagates를 합니다.  regularization effect가 다릅니다. ELD, FD는 L2 distance loss function을 사용합니다. 이것은 main training obejective인 NLL loss와 거리가 있습니다. main objective에는 log-softmax 함수가 적용되는데 이로 인해서 L2 distance 함수로 minimize하면 vector space내의 분포를 정확하게 표현하지 못합니다. KL divergence는 이와 다르게 제대로 표현 가능합니다.(Appendix C.4 참조)3    Experiments실험은 5가지 tasks에 관해서 진행함. 4 NLP, 1 CV. neural machine translation (6 datasets), abstractive summarization (1 datasets), language understanding (8 datasets), language modeling (1 dataset), image clasification (2 datasets). 편의를 위해서 R-Drop을 RD로 표기합니다.3.1    Application to Neural Machine Translation먼저 NMT tasks에 관해서 평가를 진행 합니다. 실험은 low-resource와 rich-resource에 관해서 진행 합니다.Datasetslow resource는 IWSLT14 English &lt;-&gt; German, English &lt;-&gt; Spanish, IWSLT17 English &lt;-&gt; French, English &lt;-&gt; Chinese을 사용 하였습니다.rich resource는 WMT14 English -&gt; German, English -&gt; French을 사용 하였습니다.Model &amp; Trainingmodel은 transformer은 사용 하였습니다. $\\alpha$ = 5로 세팅.Results평가는 BLEU Scores를 사용 하였습니다. IWSLT 성능은 Table 1에, WMT 결과는 Table 2에 나타 냈습니다.8개의 IWSLT에서 base 보다 2.0 BLEU 향상을 확인할 수 있습니다.  WMT의 score는 기존 SOTA model인 BERT-fused NMT(monolingual data 활용)을 뛰어 넘을 수 있었습니다.3.2    Application to Language UnderstandingDataset평가는 pre-trained models에 R-Drop을 적용하여 fine-tunning한 모델을 GLUE dataset에서 진행 하였습니다.Model &amp; TraningBERT-base, RoBERTa-large을 pre-trained models로 사용합니다. $\\alpha$는 {0.1, 0.5, 1.0} 다양한 세팅으로 진행 하였습니다.ResultsSTS-B는 Pearson correlation, CoLA는 Matthew’s correlation, 나머지 tasks는 Accuracy로 평가를 진행 하였습니다. 결과는 아래 Table 3에 나와 있습니다.결과를 보면 R-Drop을 사용한 방법은 기존 baselines인 BERT-base, RoBERTa-large에 비해서 1.21 points 와 0.80 points가 상승한 것을 확인할 수 있습니다.3.3    Application to SummarizationDatasetCNN/Daily Mail dataset을 사용 하였습니다.Model &amp; Trainingpre-trained 된 sequence-to-sequence BART model을 backbone으로 사용 하였고 fine-tune을 R-Drop을 적용해서 진행 하였습니다. $\\alpha$ 는 0.7로 설정 하였습니다.Results평가는 ROUGE F1 score로 진행 하였습니다. 실험 결과는 아래 Table 4와 같습니다.결과 base 모델보다 0.3 points 증가, SOTA를 달성할 수 있었습니다.3.4    Application to Language ModelingDatasetDataset은 Wikitext-103 dataset을 사용 하였습니다.Model &amp; Trainingmodel은 두가지를 사용 했는데 하나는 기본적인 Transformer decoder model이고, 다른 하나는 Adaptive Input Transformer(adaptive input embeddings)입니다. $\\alpha$는 1.0으로 세팅 하였습니다.Results평가 방법은 perplexity를 사용 하였습니다. 결과는 아래의 Table 5와 같습니다.결과를 보면 base로 사용한 두 모델 모두에서 R-Drop을 적용 시 성능이 향상한 것을 확인할 수 있습니다.3.5    Application to Image ClassificationDataset이미지 분류를 위한 데이터 셋은 CIFAR-100, ILSVRC-2012 ImageNet dataset을 사용 하였습니다.Model &amp; Training모델은 Vision Transformer(ViT)를 사용 하였습니다. pre-trained model을 fine-tunning 하는 과정에서 R-Drop을 적용 하였습니다. 이때 $\\alpha$는 0.6으로 설정 하였습니다.Results평가는 Accuracy로 이루어 졌습니다. 결과는 아래 Table 6에 나와 있습니다.결과를 살펴보면 성능이 증가한 것을 확인할 수 있습니다.4    Study이 섹션에서는 R-Drop method에 대해 좀 더 이해하기 위해서 다른 관점에서 광범위한 연구를 진행하였습니다. 분석 실험은 IWSLT 14 De -&gt; En translation task에서 진행 하였습니다.4.1    Regularization and Cost Analysis먼저 R-Drop의 정규화 효과를 보이고 training cost에 대한 잠제적 한계를 살펴 봅니다. training/valid loss, valid BLEU를 training update number와 training time에 따라서 plot 합니다.plot한 curves는 아래 Figure 2에 나와 있습니다.우리는 다음과 같은 사실을 관측할 수 있습니다.  training 과정을 살펴보면 transformer가 R-drop에 비해서 빠르게 over-fitting되고, train과 valid loss 간의 gap이 큰 것을 확인할 수 있습니다. 이것은 R-Drop의 regularization이 잘 작동함을 의미 합니다.  training stage의 초반부에 Transformer는 BLUE score가 빠르게 증가 합니다. 그러나 빠르게 나쁜 최적점으로 수렴 됩니다. 이와 비교해서 R-Drop는 천천히 BLEU가 증가 하지만, 더 뛰어난 성능을 달성합니다. R-Drop이 training cost를 증가 시키지만 이를 무시해도 될 정의 성능증가가 있음이 그림상에서 보여지고 있습니다.4.2    k-step R-Drop기존에는 각 step마다 R-Drop을 적용 하였지만 training efficiency를 높이기 위해서 k-step 마다 R-Drop을 적용 했을때의 효과를 알아 봅니다. 그에 대한 결과는 위의 Figure 3에 나와 있습니다.  결과를 살펴보면 k-step이 높을수록 수렴이 빠른것은 확인할 수 있지만, 더 나쁜 최적점에 도달하는 것을 볼 수 있습니다.4.3    m-time R-Drop논문에서 제안한 방법은 두 distributions인 $P_1^w(y|x)$ $P_2^w(y|x)$을 regularizes 하는 방법 입니다. 2개대신 m개를 regularizes 하면 성능이 어떻게 될지에 대해서 측정하였습니다. KL divergence에 관한 식은 아래와 같이 수정할 수 있습니다.\\(L_{KL}=\\frac{\\alpha}{m * (m-1)}\\Sigma_{i, j \\in 1,\\cdot \\cdot \\cdot, m}^{i \\not = j} D_{KL}(P_i^w(y|x)||P_j^w(y|x))\\)이를 이용해서 m=3에 대해서 실험을 진행해 본 결과 37.30을 얻을 수 있었고 m=2 일때는 37.25 였던 결과에 비해서 그렇게 성능 증가폭이 크지 않음을 확인할 수 있습니다. m=2 일때 이미 충분히 강력함을 알 수 있습니다.4.4    Two Dropout RatesIWSLT translation에서 모델의 dropout은 0.3 입니다. 이때 dropout의 영향에 대해서 알아보기 위해서 {0.1, 0.2, 0.3, 0.4, 0.5}의 dropout 집합에서 2 개씩 선택하여서 실험을 진행 하였습니다. 결과는 아래 Figure 4에 나와 있습니다.결과를 살펴보면  (0.3, 0.3)에서 가장 뛰어난 성능을 나타 냈습니다.  (0.3 ~ 0.5)의 합리적인 범위 안에서 큰 성능차이 없이 뛰어난 성능을 보였습니다.4.5    Effect of WeightKL-divergence loss weight인 $\\alpha$에 대해서 조사를 진행 하였습니다. 기존 실험은 $\\alpha$ = 5 에 관해서 진행 하였는데, 본 실험은 {1, 3, 5, 7, 10}의 범위에서 선택 하였습니다. 결과는 아래 Table 7에 나와 있습니다.결과를 살펴보면 너무 작거나 너무 클 떄 보다 5일때 제일 성능이 좋은것을 확인할 수 있습니다. 이러한 $\\alpha$는 각 task에 따라서 최적의 값이 다른데 data size나 model size가 다르기 때문입니다.5    Related WorkRegularization Methodsover fitting을 피하기 위해 많은 regularization이 제안 되었습니다. weight decay, dropout, normalization, adding noise, layer-wise pre-training and initialization, label-smoothing. 이 중 dropout이 평범한 cost로 좋은 성능을 낼 수 있어서 가장 유명합니다.Consistency Training앞에서 살펴봤던 ELD, FD, Cutoff에 대해서 이야기 하는데ELD, FD는 둘다 L2 loss function을 사용 하였고, ELD는 sub model(train 상태)와 inference 모델 간의 consistency만 신경쓰고, FD는 submodels 들간의 consistency를 고려 합니다.cutoff는 R-Drop과 비슷하게 KL-divergence를 쓰지만, sub models들 간이 아니라 문장에 빈칸을 줘서 빈칸인 sentence와 full sentence간의 consistency training을 진행 합니다.Self-distillation두개의 다른 models의 output distributions 간의 KL-divergence를 최소화 하는 것은 knowledge distilation의 student, teacher 방식과 비슷하다고 볼 수 있습니다. 다른 점이라면 R-Drop은 self knowledge distillation, 추가적인 parameter가 필요 없다는 점입니다.6    Conclusions and Future Work본 논문에서는 간단하면서도 효과적인 consistency training method, R-Drop를 제안 하였습니다. 실험결과 R-Drop은 미리 훈련된 강력한 모델뿐 아니라 large-scale datasets으로 훈련 할 때도 잘 작동하였습니다. computational resources의 제한으로 인해 몇몇 tasks는 downstream task에만 R-Drop을 적용 하였지만, pre-training을 후일에 진행할 예정 입니다."
  },
  
  {
    "title": "(논문 리뷰) BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation",
    "url": "/posts/bibert/",
    "categories": "nlp, translation",
    "tags": "deeplearning, nlp, transformer",
    "date": "2023-01-19 00:00:00 +0900",
    





    
    "snippet": "1. IntroductionPre-trained language models들은 large-scale unlabeled data에서 훈련합니다. ELMo, BERT, XLNET, XLM 등이 존재하고 다양한 NLP tasks에서 뛰어난 성능을 보이고 있습니다. 뛰어난 성능을 보이는 BERT의 pre-trained 모델을 translation model...",
    "content": "1. IntroductionPre-trained language models들은 large-scale unlabeled data에서 훈련합니다. ELMo, BERT, XLNET, XLM 등이 존재하고 다양한 NLP tasks에서 뛰어난 성능을 보이고 있습니다. 뛰어난 성능을 보이는 BERT의 pre-trained 모델을 translation model에 활용하는 몇가지의 예가 존재합니다.  Model의 Encoder 부분을 random하게 초기화 하는 대신 pre trained된 BERT의 parameter로 초기화 하는 방법  BERT의 output을 encoder의 각 layer에 활용하는 방법이 논문에서는 간단하게 pre trained language model의 output을 NMT 시스템의 input으로 활용 함으로써 IWLST’14, WMT14에서 back translation 없는 상황에서  state-of-the-art를 달성 하였습니다.또한 다양한 pre-trained langauge model을 평가하고, specialized pre-trained bilingual model을 구현 하였습니다. 추가로 두 가지 개선점을 도입 하였습니다.  stochastic layer selection  dual-directional trainingFig.1에 그에 대한 overview를 나타냅니다.2.  Contextualized Embeddings for NMT2.1 Method이 섹션에서는 pre-trained language model의 last layer(contextualized embeddings)을 활용하여 NMT model을 구성할 때의 효과를 조사 합니다.논문의 basic NMT models은 6 layer의 transformer translation models을 사용합니다. 구체적으로 source sentence를 frozen pre-trained language model에 넣어서 final layer를 NMT encoder의 embedding layer에 활용 합니다.먼저 처음으로 간단하게 contextualized embeddings을 이용할 때 얼마나 성능이 향상되는지 살펴보고, 다양한 pre-trained language models이 NMT models에 미치는 영향을 알아 봅니다.2.2  Existing Pre-Trained Models먼저 네 개의 유명한 pre-trained models(two monolingual models, two multilingual models)을 NMT모델에 활용한 것을 살펴 봅니다.  ROBERTA : BERT보다 더 큰 dataset에서 dynamic masking으로 바꾸고, next sentence prediction을 제거한다.  GottBERT : 독일어 버전의 RobertA 모델이다.  MBERT(cased) : 104개의 언어로 pre-trained된 multilingual Bert이다.  XLM-R(base) : transformer-based에서 100개의 언어로 masked language model train한 모델이다.2.3  How Do Pre-Trained LMs Affect NMT?  Dataset : low resource인 IWLST14에서 실험을 진행한다. IWLST’ 14 English-German dataset은 160k parallel bilingual sentence pairs로 구성되어 있습니다.      Settings : 모델은 six layer, FFN dimension size = 1024, attention heads = 4로 구성된 transformer_iwslt_de_en을 사용합니다. embedding dimension은 pretrained language models에 맞추기 위해서 768을 사용한다. Evaluation metric는 일반적으로 사용되는 tokenized  BLEU  (Papineni  et  al.,  2002)을 사용 합니다.        Observations : IWLST’ 14에 대한 결과는 Table 1.에 나와있다.  pre-trained language models을 활용 하려면 NMT model의 vocabulary가 pre-trained models과 동일해야 한다. random은 pre trained의 last layer을 사용하는 대신 NMT의 embedding layer을 random하게 초기화 하는 방식(기존의 방식과 동일)인데, 각 Language model마다 vocabulary가 다르기 때문에 성능 향상이 vocabulary차이 때문이 아님을 보여주기 위해서 random 방식의 실험도 같이 진행한다.decoder vocabulary size는 8K로 고정한다. 이에 대한 detail은 2.5에 나와있다. random 초기화한 결과는 대부분 비슷한 값을 가진다. embedding layer을 contextualized embeddings으로 교체한 결과 GOTTBERT와 ROBERTA의 성능은 눈에 띄게 증가 하였다. 그러나 MBERT, XLM-R은 약간 증가 하거나 오히려 성능이 감소하는 것을 확인할 수 있습니다.    Curse of Multilinguality : MBERT와 XLM-R 에서 성능이 하락하는 것을 볼 수 있는데 이는 multilinguality 때문인 것으로 보고 있다. low-resource language performance는 higher resource languages를 추가하면 증가합니다. 하지만 higher resource의 성능은 내려가게 된다. English와 German은 higher resource에 속하게 되고 성능이 하락하게 된다. 이러한 원인은 model의 parameter의 capacity가 부족해서 그렇습니다.2.4  Customized Pre-Trained LM앞에서 보았듯이 monolingual language models은 machine translation systems의 성능을 증가시키는 것을 확인할 수 있었지만 machine translation는 본질적으로 bilingual task 입니다.  따라서 논문에서는 monolingual data가 아닌 source language와 target language의 mixture data로 train 하는 방법을 가설로 제기 합니다. 이러한 bilingual pre-trained language model을 BIBERT라고 제안합니다. 이러한 BIBERT는 ROBERTa 구조를 베이스로 합니다. $BIBERT_{EN-DE}$는 GOTTBERT의 German data와 추가로 146GB English texts를 활용 합니다.2.5  Vocabulary Size Selectionencoder의 vocabulary size는 pre-trained models에 맞춰서 고정 되어 있지만, decoder의 size는 선택할 수 있습니다. low-resource machine translation에서는 decoder vocabulary size에 따라서 매우 민감합니다.  Gowda and May (2020)는 large grid search를 통해 8K BPE에서 가장 잘 작동함을 밝혔습니다. IWSLT’ 14 (160K parallel sentences) dataset에서도 적합한지 확인하기 위해 decoder vocabulary sizes (8K, 16K, 24K, 32K)에 대해서 확인 합니다.Figure 2.를 살펴보면 8K에서 가장 뛰어난 BLUE score 달성을 확인할 수 있습니다. 이후 후속 실험은 8K vocabulary sizes로 진행 합니다.2.6 BIBERT Performance  $BIBERT_{EN-DE}$ results : 앞의 Tabel 1.에서의 결과를 살펴보면 논문에서 제안한 모델은 EN -&gt; DE에서 baseline보다 2.12 증가한 29.65 score 달성함을 확인할 수 있습니다. DE -&gt; EN에서는 4.06 증가한 37.58 score를 확인할 수 있습니다. 이때 GOTTBERT와의 차이점은 extra English training data를 추가한 것 뿐이지만 1.3 BLUE point의 증가가 있었습니다.  Analysis : BIBERT의 뛰어난 성능에 대해서 분석해 보면, BIBERT의 contextualized embeddings output이 GOTTBERT보다 German information을 풍부하게 갖고 있다 생각할 수 있다. 그리고 extra English data를 가짐으로써 더 나은 assist를 translation model에 제공한다. 또한 ROBERTA 보다 더 적은 English training data를 사용하지만 0.68 BLUE의 성능 향상이 있음을 알 수 있다. BIBERT의 뛰어난 성능에 대해서 부연 설명을 하자면            비슷한 의미를 가지는 두 개의 언어에 대한 aligned embeddings for the tokens을 배울 수 있기 때문이다. 즉 source embeddings은 encoder에 aligned target embeddings에 대한 힌트를 제공할 수 있다.      overlapping된 En-De sub-word units의 Embedding은 NMT encoder에 bilingual 정보를 활용하여 translation이 가능하게 한다.      3.  Layer Selection기존에는 pre-trained models의 last layer만 활용 했었는데, BERT의 각 layer는 다른 linguistic information을 capture 합니다. 그래서 더 많은 정보를 활용하기 위해서 last layer만 말고 다른 layer도 select 하는 방법에 대해서 알아 봅니다.$\\chi$를 source language sentences의 집합이라고 나타냅니다. 각 source sentence x $\\in$ $\\chi$, $H^i_B(x)$는 x를 pre trained language model에 넣었을때 $i_{th}$ layer의 contextualize embeddings을 의미 합니다. 이때 $H^i_B(x)$는 다음과 같은 조건, last layer에서 처음 layer 까지 방향으로 K개의 layer만 고려 합니다.\\(H^i_B(x), \\,  \\forall i \\in [M - K + 1, M]\\), K는 hyperparameter, M은 pre-trained language model의 총 layers 수 입니다.3.1  Stochastic Layer Selection논문에서는 pre-trained language model의 더 많은 layer의 정보를 활용할 수 있는 stochastic layer selection을 제안함. 구체적으로 기존에는 last layer만 활용 했었는데, 각각의 batch에서 pre trained model의 layer중 랜덤하게  하나의 layer를 선택해서 NMT encoder의 input으로 사용함(Fig 3에 나타남).sentence x로 부터 NMT encoder에 들어가는 input embeddings을 $H_E(x)$로 나타내며, 훈련과정 중 다음과 같이 정해집니다.\\[H_E(x) = \\int_{i=1}^K\\mathbb{1}(\\frac {i-1}K &lt; p \\leq \\frac i K)H_B^{M-i+1}(x) \\qquad (1)\\]$\\mathbb1$은 indicator function이고 p는 [0, 1] 균등하게 분포하는 random variable 입니다. 즉 train할 때 각 batch에서 p가 [0, 1]의 범위에서 random variable로서 정해지고 indicator function는 특정 범위에 속하는 변수만 1로 나오는 함수로서,  K개의 layer 중 $(\\frac {i-1}K &lt; p \\leq \\frac i K)$에 해당하는 하나의 layer만 선택해서 train이 진행 됩니다. 이후 inference step에서는 training에 사용했던 모든 layer output의 expectation 값을 input embedding으로 사용 합니다.$\\mathbb{E}_{p \\sim uniform[0, 1]}[H_E(x)]$, 이 식은 inference 과정을 의미하고 식 1을 다음 식2와 같이 수정할 수 있습니다.\\[H_E(x) = \\frac 1 K\\sum_{i=1}^K H_B^{M-i+1}(x) \\qquad (2)\\]3.2  Experiments and ResultsTable 1에서 가장 뛰어난 성능을 보였던 $BIBERT_{EN-DE}$을 basis로 하여 후속 연구를 진행 합니다. 실험은 Section 2와 동일하게 IWSLT’ 14 dataset으로 진행합니다.Fig 4는 stochastic layer selection의 영향을 보여줍니다. K의 범위는 2~M으로 진행 하는데 $BIBERT_{EN-DE}$에서 레이어 수가 12개 이므로 M=12 입니다. K=1 이면 last layer만 select 하는것으로 section 2와 동일 하므로 제외 합니다. Fig 4를 살펴보면 모든 경우에서 K=1일때 보다 성능이 증가하는 것을 확인할 수 있습니다. K=8 일때 최대 성능을 가집니다.4. One Model, Dual-Directional Translations이 섹션에서는 평범한 one-way translation models과 다르게 dual-directional translation models에 대해서 설명 합니다. model이 En -&gt; De De -&gt; En 양방향으로 translate 할 수 있게 하는것이 목적입니다. 모델의 구조는 Section 3와 동일 합니다. 방법은 source sentence와 target sentence를 encoder, decoder에만 넣지 말고, 반대 방향과 정방향을 한번에 같은 model에서 train하는 것이 요지입니다. 이러한 방법의 motivation은 source, target sentences과 서로 contextualized representations을 향상 시켜줄 것이라고 기대하기 때문입니다. 또한 이러한 방법은 data augmentation 관점으로도 볼 수 있습니다.이 방법의 장점은 다음 2가지 입니다.  추가적인 bitexts 없이 성능 증가를 얻을 수 있습니다.  모델 구조를 따로 수정할 필요 없이 data preprocessing의 작은 수정만 필요 합니다.4.1  Dataset Preprocessingdataset은 그대로 IWSLT’14 En-&gt;De 입니다. data preprocessing 과정은 아래 Fig 5에 나와 있습니다.기존 src, tgt을 한 곳에서 concatenation후 shuffle하는 간단한 과정입니다. Decoder에서 joint English-German vocabulary size는 12k를 사용 하였습니다.4.2  Fine-tuningXu et al. (2021)에서 초기에 in and out of domain of data로 train후 in domain data로 fine-tuning시 성능이 증가한다는 점에서 영감을 받아서 초기에 Dual direction으로 train 시킨 후 mixed 데이터가 아닌 한 방향으로 fine tuning 한다는 것이 요점입니다.4.3  Experiments and Results실험 결과는 위의 Table 3에 나와 있습니다.기존의 one-way에서 dual-directional, fine-tuning, stochastic layer selection을 추가 할 수록 성능이 증가한다는 것을 나타내고 있습니다.5.  High-Resource Scenario5.1  Dataset and Training Details앞의 실험들은 low resource인 IWSLT’ 14 였고 이번 섹션에서는 high resource인 WMT’ 14 English-German dataset에 관해서 실험을 진행 합니다. Model configuration은 big transformer with 4096 FFN dimension and 16 attention heads 입니다. 여기서 $BIBERT_{EN-DE}$과 dimensions을 맞춰주기 위해서 hidden size을 1024 -&gt; 768로 바꿔 줍니다. 그리고 기존에는 low resource라 decoder의 vocabulary size를 8K와 12K를 사용 하였는데 여기서는 52K로 바꿔 줍니다.5.2  ResultsWMT’ 14에서 제공되는 bitext만 사용한 실험결과는 위의 Table 4에 나와 있습니다. 특별하게도 dual-directional translation training이 앞의 low resource와 다르게 특별한 효과를 발휘하지 못하였습니다. 하나의 가능한 시나리오는 model capacity가 mixed domain data를 handle할 정도로 충분히 크지 못하였을 가능성이 있습니다. 여기서 중요한점은 실험이 기존 모델과 다르게 hidden size를 1024 -&gt; 768로 훨씬 작은 사이즈를 했지만 기존의 결과들보다 우수한 성능을 달성한 점입니다.6.  Related Work6.1  Pre-Trained Embeddings전통적인 pre-trained embeddings은 word2vec, glove, fastText와 같은 type level로 이용 되었습니다. Peters et al.은 여기서 더 나아가서 pre-trained bidirectional LSTM의 context-aware embeddings output로 발전 시켰습니다. 그 후 attention-based transformer module이 나왔고 이어서 GPT, BERT들은 transformer model의 decoders와 encoders들을 이용해서 downstream tasks에 특화 시켰습니다. 거기서 순수한 English models에서 더 나아가서 CAMEMBERT for French, ARABERT for Arabic, Multilingual representaions MBERT, XLMS 등이 발표 되었고, cross lingual learning에 효과적임을 보였습니다. XLM-R은 대규모에서 cross lingual representation을 learning 시키고, multiple cross lingual benchmarks에서 SOTA를 달성 했습니다.6.2  MT with Context-Aware RepresentationsImamura and Sumita는 NMT의 encoder part을 지우고 BERT의 output을 directly하게 decoder에 attention mechanism을 적용 했습니다. 그들은 두 가지 단계의 optimization 과정으로 train 하였습니다. 오직 decoder만 train후 BERT를 fine-tuning 하는 방법입니다.Clinchant et al.은 NMT model의 embedding layer을 BERT embedding layer로 교체하고, encoder의 parameters를 bert의 parameter로 초기화 합니다. 그러나 여전히 NMT이 기대한 만큼 유연하지 않았습니다.Rothe et al.는 pre-trained checkpoints(e.g. BERT and GPT)를 12 layer NMT  encoder decoder를 초기화 하는데 이용 하였고, SOTA를 달성할 수 있었습니다. 흥미롭게도 GPT를 이용한 방법은 오히려 random initialized 보다 떨어졌습니다.비슷하게 Ma et al.도 transformer encoder와 decoder를 XLM-R로 초기화하고, multiple bilingual corpora에서 fine tune 하였습니다.Zhu et al.의 예비실험은 BERT output을 simple하게 이용한 것이 Bert의 parameter 값으로 초기화 한 것보다 뛰어남을 나타냅니다. 그러나 이 실험에 대해서 제한적으로 부족하게 분석 하였습니다. 그들은 주로 BERT-fuse approach 방법에 대해서 집중 하였습니다. BERT의 output을 NMT encoder, decoder에 extra multi-head attentions 합니다.Weng et al은 BERT의 last layer만 사용하는 대신 layer aware attention mechanism을 도입하여 compound contextual information을 고려하였습니다. 더 나아가서 그들은 knowledge distillation paradigm을 통해 pre-trained representation을 training process 배우는 것을 제안 하였습니다.Yaramohammadi et al은 English-Arabic translation task에 앞에서 설명한 개선점들을 모두 참조해서 적용 하였습니다. 그러나 corss lingual information extraction tasks에서 더 도움이 되는것으로 나타났습니다.7.  Conclusion본 논문에서는 mixed text of the source and target language data를 통해 훈련한 BIBERT를 NMT model에 활용합니다. BIBERT, pre-trained language model의 last layer를 단순하게 이용하는 것이 다른 방식의 활용 방법보다 더 뛰어난 결과를 나타냄을 보였습니다. 더욱이 stochastic layer selection method를 소개하여 성능 향상을 보였습니다. 마지막으로 dual direction 방식이 성능 향상에 도움이 됨을 확인 했습니다."
  }
  
]

